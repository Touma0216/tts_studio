from __future__ import annotations

import traceback
from typing import Dict, List

import numpy as np
from PyQt6.QtCore import QObject, pyqtSignal, pyqtSlot

from core.lip_sync_engine import LipSyncEngine
from core.lip_sync_engine import LipSyncData, VowelFrame
from core.tts_engine import TTSEngine
from core.audio_processor import AudioProcessor
from core.audio_effects_processor import AudioEffectsProcessor

class TTSWorker(QObject):
    """ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§TTSã¨ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯è§£æã‚’è¡Œã†ãƒ¯ãƒ¼ã‚«ãƒ¼ï¼ˆéŸ³å£°åŒæœŸå¯¾å¿œç‰ˆï¼‰"""

    synthesis_finished = pyqtSignal(object, object, object, object)

    def __init__(self, tts_engine: TTSEngine, lip_sync_engine: LipSyncEngine):
        super().__init__()
        self._tts_engine = tts_engine
        self._lip_sync_engine = lip_sync_engine

    @pyqtSlot(str, dict, bool)
    def synthesize(self, text: str, parameters: Dict, enable_lipsync: bool) -> None:
        """æŒ‡å®šã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’åˆæˆã—ã€å¿…è¦ã§ã‚ã‚Œã°ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚‚ç”Ÿæˆã™ã‚‹ï¼ˆéŸ³å£°åŒæœŸç‰ˆï¼‰"""
        try:
            # 1. TTSéŸ³å£°ç”Ÿæˆ
            sample_rate, audio = self._tts_engine.synthesize(text, **parameters)

            lipsync_data = None
            if enable_lipsync and self._lip_sync_engine and self._lip_sync_engine.is_available():
                # ğŸ”¥ ä¿®æ­£ï¼šç”Ÿæˆã•ã‚ŒãŸéŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯è§£æ
                print(f"ğŸ­ éŸ³å£°åŒæœŸãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯è§£æé–‹å§‹: {len(audio)} samples, {sample_rate}Hz")
                lipsync_data = self._lip_sync_engine.analyze_text_for_lipsync(
                    text=text,
                    audio_data=audio,
                    sample_rate=sample_rate
                )
                
                if lipsync_data:
                    print(f"âœ… ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯è§£æå®Œäº†: {len(lipsync_data.vowel_frames)}ãƒ•ãƒ¬ãƒ¼ãƒ , {lipsync_data.total_duration:.3f}ç§’")
                else:
                    print("âš ï¸ ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯è§£æå¤±æ•—")

            self.synthesis_finished.emit(sample_rate, audio, lipsync_data, None)
        except Exception:
            error_trace = traceback.format_exc()
            self.synthesis_finished.emit(None, None, None, error_trace)


class SequentialTTSWorker(QObject):
    """è¤‡æ•°ãƒ†ã‚­ã‚¹ãƒˆã‚’ã¾ã¨ã‚ã¦åˆæˆã™ã‚‹ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãƒ¯ãƒ¼ã‚«ãƒ¼"""

    sequence_finished = pyqtSignal(object, object, object, object)

    def __init__(
        self,
        tts_engine: TTSEngine,
        lip_sync_engine: LipSyncEngine,
        audio_processor: AudioProcessor,
        audio_effects_processor: AudioEffectsProcessor,
    ):
        super().__init__()
        self._tts_engine = tts_engine
        self._lip_sync_engine = lip_sync_engine
        self._audio_processor = audio_processor
        self._audio_effects_processor = audio_effects_processor

    @pyqtSlot(object, object)
    def synthesize_sequence(self, texts_data: List[Dict], options: Dict) -> None:
        """è¤‡æ•°ãƒ†ã‚­ã‚¹ãƒˆã‚’é€£ç¶šå†ç”Ÿç”¨ã«åˆæˆ"""

        try:
            if not texts_data:
                self.sequence_finished.emit(None, None, None, None)
                return

            enable_lipsync = bool(options.get('enable_lipsync', False))
            apply_cleaner = bool(options.get('apply_cleaner', False))
            cleaner_settings = options.get('cleaner_settings') or {}
            apply_effects = bool(options.get('apply_effects', False))
            effects_settings = options.get('effects_settings') or {}
            trim_threshold = float(options.get('trim_threshold', 0.0))

            all_audio = []
            combined_frames = []
            combined_texts = []
            audio_offset = 0.0
            sample_rate = None

            for entry in texts_data:
                text = (entry or {}).get('text', '').strip()
                if not text:
                    continue

                params = (entry or {}).get('parameters') or {}
                silence_after = float((entry or {}).get('silence_after', 0.0) or 0.0)

                sr, audio = self._tts_engine.synthesize(text, **params)
                if sample_rate is None:
                    sample_rate = sr

                processed_audio = audio.astype(np.float32)

                if apply_cleaner and self._audio_processor:
                    processed_audio = self._audio_processor.process_audio(processed_audio, sr, cleaner_settings)

                if apply_effects and self._audio_effects_processor:
                    processed_audio = self._audio_effects_processor.process_effects(processed_audio, sr, effects_settings)

                processed_audio = self._trim_silence(processed_audio, sr, trim_threshold)

                if processed_audio.size == 0:
                    continue

                processed_audio = processed_audio.astype(np.float32)
                segment_duration = len(processed_audio) / sr

                if enable_lipsync and self._lip_sync_engine and self._lip_sync_engine.is_available():
                    lipsync_data = self._lip_sync_engine.analyze_text_for_lipsync(
                        text=text,
                        audio_data=processed_audio,
                        sample_rate=sr,
                    )

                    if lipsync_data:
                        for frame in lipsync_data.vowel_frames:
                            combined_frames.append(
                                VowelFrame(
                                    timestamp=frame.timestamp + audio_offset,
                                    vowel=frame.vowel,
                                    intensity=frame.intensity,
                                    duration=frame.duration,
                                    is_ending=frame.is_ending,
                                )
                            )

                audio_offset += segment_duration
                combined_texts.append(text)
                all_audio.append(processed_audio)

                if silence_after > 0 and sr:
                    silence_samples = int(sr * silence_after)
                    if silence_samples > 0:
                        silence = np.zeros(silence_samples, dtype=np.float32)
                        all_audio.append(silence)

                        if enable_lipsync and self._lip_sync_engine and self._lip_sync_engine.is_available():
                            combined_frames.append(
                                VowelFrame(
                                    timestamp=audio_offset,
                                    vowel='sil',
                                    intensity=0.0,
                                    duration=silence_samples / sr,
                                    is_ending=True,
                                )
                            )

                        audio_offset += silence_samples / sr

            if not all_audio or sample_rate is None:
                self.sequence_finished.emit(None, None, None, None)
                return

            final_audio = np.concatenate(all_audio).astype(np.float32)
            max_val = float(np.abs(final_audio).max()) if final_audio.size else 0.0
            if max_val > 0.9:
                final_audio *= 0.9 / max_val

            combined_lipsync = None
            if enable_lipsync and combined_frames:
                combined_lipsync = LipSyncData(
                    text='\n'.join(combined_texts),
                    total_duration=audio_offset,
                    vowel_frames=combined_frames,
                    sample_rate=sample_rate,
                )

            self.sequence_finished.emit(sample_rate, final_audio, combined_lipsync, None)

        except Exception:
            error_trace = traceback.format_exc()
            self.sequence_finished.emit(None, None, None, error_trace)

    @staticmethod
    def _trim_silence(audio: np.ndarray, sample_rate: int, threshold: float) -> np.ndarray:
        """æœ«å°¾ã®ç„¡éŸ³ã‚’ãƒˆãƒªãƒŸãƒ³ã‚°"""

        if audio.size == 0 or threshold <= 0:
            return audio

        indices = np.where(np.abs(audio) > threshold)[0]
        if indices.size == 0:
            return audio

        end_index = indices[-1] + int(sample_rate * 0.1)
        end_index = min(end_index, audio.size)
        return audio[:end_index]