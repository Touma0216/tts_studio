"""
ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°TTSãƒ¯ãƒ¼ã‚«ãƒ¼ - ãƒãƒ£ãƒ³ã‚¯å˜ä½ã§å‡¦ç†ãƒ»é…ä¿¡
"""

import traceback
from typing import Dict, List, Optional
import numpy as np
from PyQt6.QtCore import QObject, pyqtSignal, pyqtSlot, QThread

from core.tts_engine import TTSEngine
from core.lip_sync_engine import LipSyncEngine
from core.audio_processor import AudioProcessor
from core.audio_effects_processor import AudioEffectsProcessor


class StreamingTTSWorker(QObject):
    """ãƒãƒ£ãƒ³ã‚¯å˜ä½ã§ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã™ã‚‹ãƒ¯ãƒ¼ã‚«ãƒ¼
    
    ç‰¹å¾´:
    - å…ˆè¡Œãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ï¼ˆ3ãƒãƒ£ãƒ³ã‚¯å…ˆã¾ã§ç”Ÿæˆï¼‰
    - 1ãƒãƒ£ãƒ³ã‚¯å®Œæˆã”ã¨ã«å³åº§ã«é…ä¿¡
    - ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ï¼ˆå…¨éƒ¨æºœã‚è¾¼ã¾ãªã„ï¼‰
    - ã‚­ãƒ£ãƒ³ã‚»ãƒ«å¯èƒ½
    """
    
    # ã‚·ã‚°ãƒŠãƒ«å®šç¾©
    chunk_ready = pyqtSignal(int, int, object, object)  # (index, sr, audio, lipsync)
    progress_updated = pyqtSignal(int, int)  # (current, total)
    all_finished = pyqtSignal()
    error_occurred = pyqtSignal(str)
    processing_started = pyqtSignal(int)  # (total_count)
    
    def __init__(
        self,
        tts_engine: TTSEngine,
        lip_sync_engine: LipSyncEngine,
        audio_processor: AudioProcessor,
        audio_effects_processor: AudioEffectsProcessor
    ):
        super().__init__()
        self.tts_engine = tts_engine
        self.lip_sync_engine = lip_sync_engine
        self.audio_processor = audio_processor
        self.audio_effects_processor = audio_effects_processor
        
        # è¨­å®š
        self.buffer_size = 3  # å…ˆè¡Œãƒãƒƒãƒ•ã‚¡æ•°ï¼ˆèª¿æ•´å¯èƒ½ï¼‰
        self.is_cancelled = False
        
        print("âœ… StreamingTTSWorkeråˆæœŸåŒ–å®Œäº†")
    
    @pyqtSlot(object, object)
    def synthesize_streaming(self, texts_data: List[Dict], options: Dict):
        """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ–¹å¼ã§å‡¦ç†
        
        Args:
            texts_data: ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒªã‚¹ãƒˆ
            options: å‡¦ç†ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆcleaner, effectsç­‰ï¼‰
        """
        try:
            self.is_cancelled = False
            total = len(texts_data)
            
            if total == 0:
                print("âš ï¸ å‡¦ç†ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Šã¾ã›ã‚“")
                self.all_finished.emit()
                return
            
            print(f"ğŸ¬ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†é–‹å§‹: {total}å€‹ã®ãƒ†ã‚­ã‚¹ãƒˆ")
            self.processing_started.emit(total)
            
            # ãƒãƒƒãƒ•ã‚¡ç®¡ç†
            ready_buffer = {}  # {index: (sr, audio, lipsync)}
            processing_indices = set()  # å‡¦ç†ä¸­ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            
            # æœ€åˆã®buffer_sizeå€‹ã‚’å…ˆè¡Œç”Ÿæˆé–‹å§‹
            for i in range(min(self.buffer_size, total)):
                self._start_chunk_processing_async(i, texts_data[i], options, ready_buffer, processing_indices)
            
            # é †æ¬¡é…ä¿¡ãƒ«ãƒ¼ãƒ—
            for current_index in range(total):
                if self.is_cancelled:
                    print("ğŸ›‘ ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ")
                    break
                
                # ã“ã®ãƒãƒ£ãƒ³ã‚¯ãŒæº–å‚™å®Œäº†ã™ã‚‹ã¾ã§å¾…æ©Ÿ
                while current_index not in ready_buffer:
                    if self.is_cancelled:
                        break
                    QThread.msleep(50)  # 50mså¾…æ©Ÿ
                
                if self.is_cancelled:
                    break
                
                # æº–å‚™å®Œäº†ã—ãŸãƒãƒ£ãƒ³ã‚¯ã‚’é…ä¿¡
                result = ready_buffer.pop(current_index)
                processing_indices.discard(current_index)
                
                if result is not None:
                    sr, audio, lipsync = result
                    
                    if sr is not None and audio is not None:
                        self.chunk_ready.emit(current_index, sr, audio, lipsync)
                        print(f"  âœ… [{current_index + 1}/{total}] ãƒãƒ£ãƒ³ã‚¯é…ä¿¡å®Œäº†")
                
                self.progress_updated.emit(current_index + 1, total)
                
                # æ¬¡ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ç”Ÿæˆé–‹å§‹
                next_index = current_index + self.buffer_size
                if next_index < total:
                    self._start_chunk_processing_async(
                        next_index, 
                        texts_data[next_index], 
                        options, 
                        ready_buffer,
                        processing_indices
                    )
            
            if not self.is_cancelled:
                print("âœ… ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†å®Œäº†")
                self.all_finished.emit()
            
        except Exception as e:
            error_trace = traceback.format_exc()
            print(f"âŒ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã‚¨ãƒ©ãƒ¼:\n{error_trace}")
            self.error_occurred.emit(error_trace)
    
    def _start_chunk_processing_async(
        self, 
        index: int, 
        entry: Dict, 
        options: Dict,
        result_buffer: Dict,
        processing_indices: set
    ):
        """1ãƒãƒ£ãƒ³ã‚¯ã‚’éåŒæœŸã§å‡¦ç†é–‹å§‹ã—ã¦ãƒãƒƒãƒ•ã‚¡ã«æ ¼ç´
        
        Args:
            index: ãƒãƒ£ãƒ³ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            entry: ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
            options: å‡¦ç†ã‚ªãƒ—ã‚·ãƒ§ãƒ³
            result_buffer: çµæœæ ¼ç´ç”¨ãƒãƒƒãƒ•ã‚¡
            processing_indices: å‡¦ç†ä¸­ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ã‚»ãƒƒãƒˆ
        """
        try:
            text = entry.get('text', '').strip()
            if not text:
                result_buffer[index] = None
                return
            
            processing_indices.add(index)
            
            # åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å‡¦ç†ï¼ˆå®Ÿéš›ã«ã¯ã“ã®ãƒ¯ãƒ¼ã‚«ãƒ¼è‡ªä½“ãŒåˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ãªã®ã§ç›´æ¥å®Ÿè¡Œï¼‰
            result = self._process_single_chunk(index, entry, options)
            result_buffer[index] = result
            
        except Exception as e:
            print(f"âŒ ãƒãƒ£ãƒ³ã‚¯{index}å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            result_buffer[index] = None
    
    def _process_single_chunk(self, index: int, entry: Dict, options: Dict) -> Optional[tuple]:
        """1ãƒãƒ£ãƒ³ã‚¯ã‚’å‡¦ç†
        
        Returns:
            (sample_rate, audio, lipsync_data) or None
        """
        try:
            text = entry.get('text', '').strip()
            if not text:
                return None
            
            params = entry.get('parameters', {})
            
            print(f"  ğŸ”„ [{index + 1}] å‡¦ç†é–‹å§‹: '{text[:30]}...'")
            
            # 1. TTSç”Ÿæˆ
            sr, audio = self.tts_engine.synthesize(text, **params)
            
            if audio is None or sr is None:
                print(f"  âš ï¸ [{index + 1}] TTSç”Ÿæˆå¤±æ•—")
                return None
            
            # 2. éŸ³å£°å‡¦ç†ï¼ˆã‚¯ãƒªãƒ¼ãƒŠãƒ¼ï¼‰
            if options.get('apply_cleaner') and self.audio_processor:
                cleaner_settings = options.get('cleaner_settings', {})
                audio = self.audio_processor.process_audio(audio, sr, cleaner_settings)
            
            # 3. éŸ³å£°ã‚¨ãƒ•ã‚§ã‚¯ãƒˆ
            if options.get('apply_effects') and self.audio_effects_processor:
                effects_settings = options.get('effects_settings', {})
                audio = self.audio_effects_processor.process_effects(audio, sr, effects_settings)
            
            # 4. ç„¡éŸ³ãƒˆãƒªãƒŸãƒ³ã‚°
            trim_threshold = float(options.get('trim_threshold', 0.0))
            if trim_threshold > 0:
                audio = self._trim_silence(audio, sr, trim_threshold)
            
            # 5. ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯ç”Ÿæˆ
            lipsync = None
            if options.get('enable_lipsync') and self.lip_sync_engine.is_available():
                lipsync = self.lip_sync_engine.analyze_text_for_lipsync(
                    text=text,
                    audio_data=audio,  # ğŸ‘ˆ ç„¡éŸ³è¿½åŠ å‰ã®éŸ³å£°ã§è§£æã—ã¦ã‚‹
                    sample_rate=sr
                )

            # 6. silence_after ã‚’åæ˜ 
            silence_after = float(entry.get('silence_after', 0.0) or 0.0)
            if silence_after > 0:
                silence_samples = int(sr * silence_after)
                silence_padding = np.zeros(silence_samples, dtype=np.float32)
                audio = np.concatenate([audio, silence_padding])  # ğŸ‘ˆ éŸ³å£°ã«ç„¡éŸ³è¿½åŠ 
                
                # ğŸ†• ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯ã«ã‚‚ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ è¿½åŠ 
                if lipsync:
                    from core.lip_sync_engine import VowelFrame
                    lipsync.vowel_frames.append(
                        VowelFrame(
                            timestamp=lipsync.total_duration,
                            vowel='sil',
                            intensity=0.0,
                            duration=silence_after,
                            is_ending=True
                        )
                    )
                    lipsync.total_duration += silence_after
            print(f"  âœ… [{index + 1}] å‡¦ç†å®Œäº†: {len(audio)/sr:.2f}ç§’")
            
            return (sr, audio, lipsync)
            
        except Exception as e:
            print(f"âŒ ãƒãƒ£ãƒ³ã‚¯{index}å‡¦ç†ä¸­ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return None

    
    @staticmethod
    def _trim_silence(audio: np.ndarray, sample_rate: int, threshold: float) -> np.ndarray:
        """æœ«å°¾ã®ç„¡éŸ³ã‚’ãƒˆãƒªãƒŸãƒ³ã‚°"""
        if audio.size == 0 or threshold <= 0:
            return audio
        
        indices = np.where(np.abs(audio) > threshold)[0]
        if indices.size == 0:
            return audio
        
        end_index = indices[-1] + int(sample_rate * 0.1)
        end_index = min(end_index, audio.size)
        return audio[:end_index]
    
    def cancel(self):
        """å‡¦ç†ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«"""
        print("ğŸ›‘ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«")
        self.is_cancelled = True