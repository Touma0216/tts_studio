import torch
import numpy as np
from pathlib import Path
import traceback
import inspect
import logging
import json
from typing import List, Dict, Optional
from scipy.signal import butter, filtfilt, iirnotch
from scipy.ndimage import gaussian_filter1d
from scipy.fft import fft, ifft

# Style-Bert-VITS2ã®ãƒ­ã‚°ã‚’ç„¡åŠ¹åŒ–
logging.getLogger("style_bert_vits2").setLevel(logging.ERROR)
logging.getLogger("bert_models").setLevel(logging.ERROR)  
logging.getLogger("tts_model").setLevel(logging.ERROR)
logging.getLogger("infer").setLevel(logging.ERROR)
logging.getLogger("transformers").setLevel(logging.ERROR)
logging.getLogger("torch").setLevel(logging.ERROR)

class TTSEngine:
    def __init__(self):
        self.model = None
        self.is_loaded = False
        self.model_info = {}
        self._use_autocast = torch.cuda.is_available()

        # core/tts_engine.py ã® default_params ã‚’ä¿®æ­£
        self.default_params = {
            'style': 'Neutral',
            'style_weight': 1.0,
            'sdp_ratio': 0.5,    # ğŸ‘ˆ 0.02 â†’ 0.25ï¼ˆå…ƒã®å€¤ï¼‰
            'noise': 0.6,
            'noise_w': 0.8,        # ğŸ‘ˆ 0.02 â†’ 0.25ï¼ˆå…ƒã®å€¤ï¼‰
            'length_scale': 1.0
        }

        # å¾Œå‡¦ç†ã‚’å…¨éƒ¨ç„¡åŠ¹ã«ã™ã‚‹ï¼ˆå•é¡Œåˆ‡ã‚Šåˆ†ã‘ã®ãŸã‚ï¼‰
        self.audio_processing = {
            'normalize': True,
            'target_peak_db': -9.0,        
            'remove_hum': True,           
            'remove_dc': True,
            'soft_limit': False,           # ğŸ‘ˆ ä¸€æ™‚ç„¡åŠ¹
            'limit_threshold': 0.95,
            'spectral_cleaning': False,    # ğŸ‘ˆ ä¸€æ™‚ç„¡åŠ¹
            'professional_gate': False,    # ğŸ‘ˆ ä¸€æ™‚ç„¡åŠ¹
            'frequency_cleanup': False,    
        }
        
        # æ„Ÿæƒ…åãƒãƒƒãƒ”ãƒ³ã‚°
        self.emotion_mapping = {
            'fear': 'Fear', 'angry': 'Angry', 'disgust': 'Disgust',
            'happiness': 'Happy', 'happy': 'Happy', 'sadness': 'Sad',
            'sad': 'Sad', 'surprise': 'Surprise', 'neutral': 'Neutral',
            'Fear': 'Fear', 'Angry': 'Angry', 'Disgust': 'Disgust', 
            'Happy': 'Happy', 'Sad': 'Sad', 'Surprise': 'Surprise',
            'Neutral': 'Neutral',
        }
        self.long_processor = None
    
    def remove_dc_offset(self, audio):
        """DCã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’é™¤å»"""
        return audio - np.mean(audio)
    
    def professional_noise_gate(self, audio, sr):
        """ãƒ—ãƒ­ä»•æ§˜ãƒã‚¤ã‚ºã‚²ãƒ¼ãƒˆ"""
        if len(audio) < 1024:
            return audio
            
        try:
            # RMSãƒ™ãƒ¼ã‚¹ã®ã‚ˆã‚Šæ­£ç¢ºãªã‚²ãƒ¼ãƒˆ
            window_size = int(sr * 0.01)  # 10msçª“
            hop_size = window_size // 2
            
            # å…¨ä½“ã®RMSè¨ˆç®—
            total_rms = np.sqrt(np.mean(audio**2))
            
            # å³ã—ã„ã‚²ãƒ¼ãƒˆé–¾å€¤ï¼ˆå•†ç”¨å“è³ªã®ãŸã‚ï¼‰
            gate_threshold = total_rms * 0.02  # 2%
            
            gated_audio = audio.copy()
            gate_applied = 0
            
            for i in range(0, len(audio) - window_size, hop_size):
                frame = audio[i:i+window_size]
                frame_rms = np.sqrt(np.mean(frame**2))
                
                if frame_rms < gate_threshold:
                    # æ®µéšçš„æ¸›è¡°ï¼ˆå®Œå…¨ãƒŸãƒ¥ãƒ¼ãƒˆã§ã¯ãªãï¼‰
                    reduction = 0.005  # 0.5%ã¾ã§æ¸›è¡°
                    gated_audio[i:i+window_size] *= reduction
                    gate_applied += 1
            
            gate_percentage = (gate_applied * hop_size / len(audio)) * 100
            print(f"ğŸšª ãƒ—ãƒ­ä»•æ§˜ã‚²ãƒ¼ãƒˆ: {gate_percentage:.1f}%å‡¦ç†, é–¾å€¤={gate_threshold:.6f}")
            
            return gated_audio
            
        except Exception as e:
            print(f"âš ï¸ ãƒã‚¤ã‚ºã‚²ãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return audio
    
    def frequency_cleanup(self, audio, sr):
        """å•é¡Œå‘¨æ³¢æ•°ã®å¾¹åº•æ¸…æƒ"""
        try:
            cleaned = audio.copy()
            nyquist = sr / 2
            
            # 1. ã€Œã˜ãƒ¼ãƒ¼ã€éŸ³ã®åŸå› ã¨ãªã‚‹æŒç¶šæ€§ãƒˆãƒ¼ãƒ³ã‚’é™¤å»
            problem_frequencies = [
                # ä½åŸŸã®æŒç¶šéŸ³
                50, 60, 100, 120, 150, 180, 200, 240, 300,
                # ä¸­åŸŸã®ã€Œã˜ãƒ¼ãƒ¼ã€éŸ³
                800, 1000, 1200, 1500, 1800, 2000, 2200, 2500,
                # é«˜åŸŸã®ãƒ‡ã‚¸ã‚¿ãƒ«ãƒã‚¤ã‚º
                3000, 3500, 4000, 4500, 5000, 5500, 6000, 6500, 7000
            ]
            
            processed_count = 0
            for freq in problem_frequencies:
                if freq < nyquist * 0.95:
                    # è¶…ç‹­å¸¯åŸŸãƒãƒƒãƒãƒ•ã‚£ãƒ«ã‚¿
                    Q = 50  # éå¸¸ã«é‹­ã„ãƒ•ã‚£ãƒ«ã‚¿
                    b, a = iirnotch(freq, Q, sr)
                    cleaned = filtfilt(b, a, cleaned)
                    processed_count += 1
            
            print(f"ğŸ¯ å‘¨æ³¢æ•°æ¸…æƒ: {processed_count}ç®‡æ‰€å‡¦ç†")
            
            # 2. 6kHzä»¥ä¸Šã®é«˜å‘¨æ³¢ã‚«ãƒƒãƒˆï¼ˆã‚ˆã‚Šç©æ¥µçš„ï¼‰
            cutoff = min(6000, nyquist * 0.8)
            cutoff_norm = cutoff / nyquist
            
            if cutoff_norm < 0.95:
                # 8æ¬¡ãƒ•ã‚£ãƒ«ã‚¿ã§ã‚ˆã‚Šæ€¥å³»ã«
                b, a = butter(8, cutoff_norm, btype='low')
                cleaned = filtfilt(b, a, cleaned)
                print(f"ğŸ”‡ å¼·åŠ›ãƒ­ãƒ¼ãƒ‘ã‚¹: {cutoff}Hz (8æ¬¡)")
            
            return cleaned
            
        except Exception as e:
            print(f"âš ï¸ å‘¨æ³¢æ•°æ¸…æƒã‚¨ãƒ©ãƒ¼: {e}")
            return audio
    
    def spectral_cleaning(self, audio, sr):
        """é«˜åº¦ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«æ¸…æµ„åŒ–ï¼ˆaudio_processor.pyé¢¨ï¼‰"""
        try:
            if len(audio) < 2048:
                return audio
                
            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            frame_size = 2048
            hop_size = 512
            window = np.hanning(frame_size)
            
            # ãƒ•ãƒ¬ãƒ¼ãƒ æ•°è¨ˆç®—
            num_frames = (len(audio) - frame_size) // hop_size + 1
            if num_frames <= 0:
                return audio
            
            # ãƒã‚¤ã‚ºãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«æ¨å®šï¼ˆæœ€åˆã®5ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰
            noise_profile = self._estimate_noise_profile(audio, frame_size, hop_size, window)
            
            # å‡¦ç†æ¸ˆã¿éŸ³å£°ãƒãƒƒãƒ•ã‚¡
            cleaned = np.zeros_like(audio)
            
            for i in range(num_frames):
                start = i * hop_size
                end = start + frame_size
                
                if end > len(audio):
                    break
                
                # ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡º
                frame = audio[start:end] * window
                
                # FFT
                spectrum = fft(frame)
                magnitude = np.abs(spectrum)
                phase = np.angle(spectrum)
                
                # ç©æ¥µçš„ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«ã‚µãƒ–ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³
                cleaned_magnitude = self._aggressive_spectral_subtraction(magnitude, noise_profile)
                
                # é€†FFT
                cleaned_spectrum = cleaned_magnitude * np.exp(1j * phase)
                cleaned_frame = np.real(ifft(cleaned_spectrum))
                
                # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚¢ãƒ‰
                cleaned[start:end] += cleaned_frame * window
            
            print(f"ğŸŒŠ ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«æ¸…æµ„åŒ–: {num_frames}ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†")
            return cleaned.astype(np.float32)
            
        except Exception as e:
            print(f"âš ï¸ ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«æ¸…æµ„åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return audio
    
    def _estimate_noise_profile(self, audio, frame_size, hop_size, window):
        """ãƒã‚¤ã‚ºãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«æ¨å®š"""
        noise_frames = min(5, (len(audio) - frame_size) // hop_size)
        
        if noise_frames <= 0:
            spectrum = np.abs(fft(audio[:frame_size] * window))
            return spectrum * 0.05  # éå¸¸ã«æ§ãˆã‚
        
        noise_spectra = []
        for i in range(noise_frames):
            start = i * hop_size
            end = start + frame_size
            frame = audio[start:end] * window
            spectrum = np.abs(fft(frame))
            noise_spectra.append(spectrum)
        
        # ã‚ˆã‚Šä¿å®ˆçš„ãªãƒã‚¤ã‚ºãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
        noise_profile = np.percentile(noise_spectra, 20, axis=0)  # ä¸‹ä½20%
        return noise_profile
    
    def _aggressive_spectral_subtraction(self, magnitude, noise_profile):
        """ç©æ¥µçš„ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«ã‚µãƒ–ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³"""
        # SNRè¨ˆç®—
        snr = magnitude / (noise_profile + 1e-12)
        
        # 3æ®µéšã®ç©æ¥µçš„ã‚µãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³
        suppression = np.ones_like(snr)
        
        # ç¬¬1æ®µéšï¼šä¸­ç¨‹åº¦ã®ãƒã‚¤ã‚º
        mask1 = snr < 5.0
        suppression[mask1] = 0.3
        
        # ç¬¬2æ®µéšï¼šå¼·ã„ãƒã‚¤ã‚º
        mask2 = snr < 2.0
        suppression[mask2] = 0.1
        
        # ç¬¬3æ®µéšï¼šæ¥µã‚ã¦å¼·ã„ãƒã‚¤ã‚º
        mask3 = snr < 1.2
        suppression[mask3] = 0.01
        
        # ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°
        suppression = gaussian_filter1d(suppression, sigma=1.5)
        
        # é©ç”¨
        cleaned_magnitude = magnitude * suppression
        
        # æœ€å°ãƒ¬ãƒ™ãƒ«åˆ¶é™
        min_magnitude = magnitude * 0.001  # 0.1%ã¾ã§
        cleaned_magnitude = np.maximum(cleaned_magnitude, min_magnitude)
        
        return cleaned_magnitude
    
    def normalize_audio(self, audio, target_peak_db=-9.0):
        """éŸ³å£°æ­£è¦åŒ–ï¼ˆä¿å®ˆçš„ï¼‰"""
        audio = self.remove_dc_offset(audio)
        
        current_peak = np.max(np.abs(audio))
        if current_peak == 0:
            return audio
        
        target_peak_linear = 10 ** (target_peak_db / 20.0)
        scale_factor = target_peak_linear / current_peak
        normalized = audio * scale_factor
        
        print(f"ğŸ”Š éŸ³é‡æ­£è¦åŒ–: {current_peak:.3f} -> {np.max(np.abs(normalized)):.3f}")
        return normalized
    
    def soft_limiter(self, audio, threshold=0.9):
        """ã‚½ãƒ•ãƒˆãƒªãƒŸãƒƒã‚¿ãƒ¼ï¼ˆä¿å®ˆçš„ï¼‰"""
        abs_audio = np.abs(audio)
        mask = abs_audio > threshold
        
        if np.any(mask):
            sign = np.sign(audio)
            limited = np.where(
                mask,
                sign * threshold * np.tanh(abs_audio / threshold),
                audio
            )
            
            clipped_samples = np.sum(mask)
            clip_rate = (clipped_samples / len(audio)) * 100
            print(f"ğŸ›¡ï¸ ã‚½ãƒ•ãƒˆãƒªãƒŸãƒƒã‚¿ãƒ¼: {clip_rate:.2f}%å‡¦ç†")
            return limited
        
        return audio
    
    def process_audio(self, audio, sr):
        """ç·åˆéŸ³å£°å‡¦ç†ï¼ˆå•†ç”¨å“è³ªç‰ˆï¼‰"""
        processed = audio.copy()
        
        # Float32å¤‰æ›
        if processed.dtype != np.float32:
            if processed.dtype == np.int16:
                processed = processed.astype(np.float32) / 32768.0
            elif processed.dtype == np.int32:
                processed = processed.astype(np.float32) / 2147483648.0
            else:
                processed = processed.astype(np.float32)
        
        original_peak = np.max(np.abs(processed))
        print(f"ğŸµ å…ƒéŸ³å£°ãƒ”ãƒ¼ã‚¯: {original_peak:.6f}")
        
        # 1. DCã‚ªãƒ•ã‚»ãƒƒãƒˆé™¤å»
        if self.audio_processing['remove_dc']:
            processed = self.remove_dc_offset(processed)
        
        # 2. å‘¨æ³¢æ•°æ¸…æƒï¼ˆæœ€å„ªå…ˆï¼‰
        if self.audio_processing.get('frequency_cleanup', True):
            processed = self.frequency_cleanup(processed, sr)
        
        # 3. ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«æ¸…æµ„åŒ–
        if self.audio_processing.get('spectral_cleaning', True):
            processed = self.spectral_cleaning(processed, sr)
        
        # 4. ãƒ—ãƒ­ä»•æ§˜ãƒã‚¤ã‚ºã‚²ãƒ¼ãƒˆ
        if self.audio_processing.get('professional_gate', True):
            processed = self.professional_noise_gate(processed, sr)
        
        # 5. éŸ³é‡æ­£è¦åŒ–
        if self.audio_processing['normalize']:
            processed = self.normalize_audio(
                processed, 
                self.audio_processing['target_peak_db']
            )
        
        # 6. ã‚½ãƒ•ãƒˆãƒªãƒŸãƒƒã‚¿ãƒ¼
        if self.audio_processing['soft_limit']:
            processed = self.soft_limiter(
                processed,
                self.audio_processing['limit_threshold']
            )
        
        final_peak = np.max(np.abs(processed))
        print(f"ğŸ¶ å‡¦ç†å¾Œãƒ”ãƒ¼ã‚¯: {final_peak:.6f}")
        
        return processed
    
    def load_model(self, model_path, config_path, style_path):
        """ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿"""
        try:
            import sys
            from io import StringIO
            
            old_stdout = sys.stdout
            old_stderr = sys.stderr
            sys.stdout = StringIO()
            sys.stderr = StringIO()
            
            try:
                from style_bert_vits2.nlp import bert_models
                from style_bert_vits2.constants import Languages
                from style_bert_vits2.tts_model import TTSModel
                
                bert_models.load_model(Languages.JP, "ku-nlp/deberta-v2-large-japanese-char-wwm")
                bert_models.load_tokenizer(Languages.JP, "ku-nlp/deberta-v2-large-japanese-char-wwm")
                
                device = "cuda" if torch.cuda.is_available() else "cpu"
                
                self.model = TTSModel(
                    model_path=model_path,
                    config_path=config_path,
                    style_vec_path=style_path,
                    device=device,
                )
                
            finally:
                sys.stdout = old_stdout
                sys.stderr = old_stderr

            self._inference_device = device
            self._use_autocast = device == "cuda"

            
            self.model_info = {
                'model_path': model_path,
                'config_path': config_path,
                'style_path': style_path,
                'device': device
            }
            
            self.is_loaded = True
            self._update_emotion_mapping()
            
            print(f"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {Path(model_path).parent.name}")
            print(f"ğŸ“± åˆ©ç”¨å¯èƒ½ãªæ„Ÿæƒ…: {list(self.get_available_styles())}")
            
            return True
            
        except Exception as e:
            if 'old_stdout' in locals():
                sys.stdout = old_stdout
                sys.stderr = old_stderr
            
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            self.is_loaded = False
            return False
    
    def _update_emotion_mapping(self):
        """æ„Ÿæƒ…ãƒãƒƒãƒ”ãƒ³ã‚°æ›´æ–°"""
        try:
            actual_styles = self._get_actual_styles_from_model()
            print(f"ğŸ” ãƒ¢ãƒ‡ãƒ«å†…ã®å®Ÿéš›ã®æ„Ÿæƒ…: {actual_styles}")
            
            # æ—¢å­˜ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ã‚¯ãƒªã‚¢
            self.emotion_mapping = {}
            
            # KMeansãƒ™ãƒ¼ã‚¹ã®ã‚¹ã‚¿ã‚¤ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°
            if 'Style_0' in actual_styles:
                # Style_0 = Neutral (é€šå¸¸ã¯æœ€åˆã®ã‚¯ãƒ©ã‚¹ã‚¿)
                self.emotion_mapping['neutral'] = 'Style_0'
                self.emotion_mapping['Neutral'] = 'Style_0'
                
                # ä»–ã®ã‚¹ã‚¿ã‚¤ãƒ«ã‚’æ„Ÿæƒ…ã«å‰²ã‚Šå½“ã¦
                if len(actual_styles) >= 2:
                    self.emotion_mapping['fear'] = 'Style_1'
                    self.emotion_mapping['Fear'] = 'Style_1'
                if len(actual_styles) >= 3:
                    self.emotion_mapping['sadness'] = 'Style_2'
                    self.emotion_mapping['Sad'] = 'Style_2'
                if len(actual_styles) >= 4:
                    self.emotion_mapping['surprise'] = 'Style_3'
                    self.emotion_mapping['Surprise'] = 'Style_3'
            else:
                # å¾“æ¥ã®æ„Ÿæƒ…åã®å ´åˆ
                for actual_style in actual_styles:
                    self.emotion_mapping[actual_style] = actual_style
                    self.emotion_mapping[actual_style.lower()] = actual_style
            
            print(f"ğŸ”„ æ›´æ–°ã•ã‚ŒãŸæ„Ÿæƒ…ãƒãƒƒãƒ”ãƒ³ã‚°: {self.emotion_mapping}")
            
        except Exception as e:
            print(f"âš ï¸ æ„Ÿæƒ…ãƒãƒƒãƒ”ãƒ³ã‚°æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _get_actual_styles_from_model(self):
        """å®Ÿéš›ã®åˆ©ç”¨å¯èƒ½æ„Ÿæƒ…ã‚’å–å¾—"""
        try:
            config_path = self.model_info.get('config_path')
            if config_path and Path(config_path).exists():
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                
                if 'data' in config and 'style2id' in config['data']:
                    style2id = config['data']['style2id']
                    if style2id:
                        emotions = list(style2id.keys())
                        print(f"ğŸ­ config.jsonã‹ã‚‰æ„Ÿæƒ…ç™ºè¦‹: {emotions}")
                        return emotions
            
            return ["Neutral"]
            
        except Exception as e:
            print(f"âŒ æ„Ÿæƒ…å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return ["Neutral"]
    
    def get_available_styles(self):
        """åˆ©ç”¨å¯èƒ½æ„Ÿæƒ…å–å¾—"""
        if not self.is_loaded:
            return ["Neutral"]
        return self._get_actual_styles_from_model()
    
    def normalize_emotion(self, emotion):
        """æ„Ÿæƒ…åæ­£è¦åŒ–"""
        if not emotion:
            return 'Neutral'
        
        normalized = self.emotion_mapping.get(emotion, emotion)
        if normalized != emotion:
            print(f"ğŸ”„ æ„Ÿæƒ…æ­£è¦åŒ–: '{emotion}' -> '{normalized}'")
        
        return normalized
    
    def synthesize(self, text, **params):
        """éŸ³å£°åˆæˆå®Ÿè¡Œï¼ˆæ ¹æœ¬è§£æ±ºç‰ˆï¼‰"""
        if not self.is_loaded or self.model is None:
            raise RuntimeError("ãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
        
        if not text.strip():
            raise ValueError("ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ã™")
            
        try:
            import sys
            from io import StringIO
            
            old_stdout = sys.stdout
            old_stderr = sys.stderr
            sys.stdout = StringIO()
            sys.stderr = StringIO()
            
            try:
                # ğŸ”¥ è¶…ä½ãƒã‚¤ã‚ºãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¼·åˆ¶é©ç”¨
                synth_params = self.default_params.copy()
                synth_params.update(params)
                
                # æ„Ÿæƒ…æ­£è¦åŒ–
                if 'style' in synth_params:
                    original_style = synth_params['style']
                    synth_params['style'] = self.normalize_emotion(original_style)
                    
                    if original_style != synth_params['style']:
                        print(f"âœ¨ æ„Ÿæƒ…æ­£è¦åŒ–: {original_style} â†’ {synth_params['style']}")
                
                # æ¨è«–å®Ÿè¡Œ
                kwargs = self._build_infer_kwargs(text, synth_params)

                sr = raw_audio = None
                autocast_enabled = torch.cuda.is_available() and getattr(self, "_use_autocast", False)

                if autocast_enabled:
                    try:
                        with torch.cuda.amp.autocast():
                            sr, raw_audio = self.model.infer(**kwargs)
                    except (AssertionError, RuntimeError, FloatingPointError) as amp_error:
                        logging.getLogger(__name__).warning(
                            "AMP inference failed, retrying without autocast: %s", amp_error
                        )
                        self._use_autocast = False
                        sr, raw_audio = self.model.infer(**kwargs)
                else:
                    sr, raw_audio = self.model.infer(**kwargs)            
            finally:
                sys.stdout = old_stdout
                sys.stderr = old_stderr
            if isinstance(raw_audio, torch.Tensor):
                raw_audio = raw_audio.detach().cpu().float().numpy()
            
            if raw_audio is None or len(raw_audio) == 0:
                raise RuntimeError("éŸ³å£°ãƒ‡ãƒ¼ã‚¿ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            
            # å•†ç”¨å“è³ªå¾Œå‡¦ç†
            print(f"ğŸµ éŸ³å£°å¾Œå‡¦ç†é–‹å§‹...")
            processed_audio = self.process_audio(raw_audio, sr)
            print(f"âœ… éŸ³å£°å¾Œå‡¦ç†å®Œäº†")
                
            return sr, processed_audio
            
        except Exception as e:
            print(f"âŒ éŸ³å£°åˆæˆã‚¨ãƒ©ãƒ¼: {e}")
            raise e
    
    def _build_infer_kwargs(self, text, params):
        """æ¨è«–å¼•æ•°æ§‹ç¯‰"""
        if not self.model:
            raise RuntimeError("ãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            
        sig = inspect.signature(self.model.infer)
        method_params = sig.parameters
        
        kwargs = {}
        if "text" in method_params:
            kwargs["text"] = text
        else:
            first_param = next(iter(method_params))
            kwargs[first_param] = text
        
        # ã‚¹ã‚¿ã‚¤ãƒ«ç³»
        if "style" in method_params:
            kwargs["style"] = params.get('style', 'Neutral')
        if "style_weight" in method_params:
            kwargs["style_weight"] = params.get('style_weight', 1.0)
        elif "emotion_weight" in method_params:
            kwargs["emotion_weight"] = params.get('style_weight', 1.0)

        # é•·ã•ç³»
        length_scale = params.get('length_scale', 0.85)
        if "length_scale" in method_params:
            kwargs["length_scale"] = length_scale
        elif "duration_scale" in method_params:
            kwargs["duration_scale"] = length_scale
        elif "speed" in method_params:
            kwargs["speed"] = 1.0 / length_scale
        elif "length" in method_params:
            kwargs["length"] = length_scale
        
        # SDP
        sdp_value = params.get('sdp_ratio', 0.05)  # ğŸ”¥ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ0.05
        if "sdp_ratio" in method_params:
            kwargs["sdp_ratio"] = sdp_value
        elif "sdp" in method_params:
            kwargs["sdp"] = sdp_value
        
        # ãƒã‚¤ã‚ºç³»
        noise_value = params.get('noise', 0.05)  # ğŸ”¥ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ0.05
        if "noise" in method_params:
            kwargs["noise"] = noise_value
        elif "noise_scale_w" in method_params:
            kwargs["noise_scale_w"] = noise_value
        elif "noise_scale" in method_params:
            kwargs["noise_scale"] = noise_value
        
        # ãã®ä»–
        if "pitch_scale" in method_params:
            kwargs["pitch_scale"] = params.get('pitch_scale', 1.0)
        if "intonation_scale" in method_params:
            kwargs["intonation_scale"] = params.get('intonation_scale', 1.0)
        
        print(f"ğŸ”§ æ¨è«–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {kwargs}")
        return kwargs
    
    def set_audio_processing(self, **settings):
        """éŸ³å£°å‡¦ç†è¨­å®šå¤‰æ›´"""
        for key, value in settings.items():
            if key in self.audio_processing:
                old_value = self.audio_processing[key]
                self.audio_processing[key] = value
                print(f"ğŸ”§ è¨­å®šå¤‰æ›´: {key} = {old_value} -> {value}")
            else:
                print(f"âš ï¸ æœªçŸ¥ã®è¨­å®š: {key}")
    
    def get_audio_processing_settings(self):
        """éŸ³å£°å‡¦ç†è¨­å®šå–å¾—"""
        return self.audio_processing.copy()
    
    def get_model_info(self):
        """ãƒ¢ãƒ‡ãƒ«æƒ…å ±å–å¾—"""
        return self.model_info.copy() if self.is_loaded else {}
    
    def unload_model(self):
        """ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        if self.model:
            del self.model
            self.model = None
        self.is_loaded = False
        self.model_info = {}
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    def debug_emotions(self):
        """æ„Ÿæƒ…ãƒ‡ãƒãƒƒã‚°"""
        print("\n=== æ„Ÿæƒ…ãƒãƒƒãƒ”ãƒ³ã‚° ãƒ‡ãƒãƒƒã‚° ===")
        print(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {'âœ…' if self.is_loaded else 'âŒ'}")
        
        if self.is_loaded:
            actual_styles = self._get_actual_styles_from_model()
            print(f"åˆ©ç”¨å¯èƒ½æ„Ÿæƒ…: {actual_styles}")
        
        print("æ„Ÿæƒ…ãƒãƒƒãƒ”ãƒ³ã‚°:")
        for key, value in sorted(self.emotion_mapping.items()):
            status = "âœ…" if key == value else f"ğŸ”„ -> {value}"
            print(f"  '{key}' {status}")
        print("=== ãƒ‡ãƒãƒƒã‚°çµ‚äº† ===\n")
    
    def test_emotion(self, emotion, text="ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆã§ã™"):
        """æ„Ÿæƒ…ãƒ†ã‚¹ãƒˆ"""
        try:
            print(f"ğŸ§ª æ„Ÿæƒ…ãƒ†ã‚¹ãƒˆ: '{emotion}'")
            normalized = self.normalize_emotion(emotion)
            print(f"ğŸ“ æ­£è¦åŒ–å¾Œ: '{normalized}'")
            
            sr, audio = self.synthesize(text, style=emotion, style_weight=1.0)
            print(f"âœ… ãƒ†ã‚¹ãƒˆæˆåŠŸ: {len(audio)} samples, {sr}Hz")
            return True, sr, audio
            
        except Exception as e:
            print(f"âŒ ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
            return False, None, None
        
    def generate_continuous_wav(
        self, 
        texts_data: List[Dict],
        output_path: str,
        chunk_size: int = 100,
        resume: bool = True,
        progress_callback = None
    ) -> Dict:
        """
        è¤‡æ•°ãƒ†ã‚­ã‚¹ãƒˆã‚’é€£ç¶šTTSå‡¦ç†ã—ã¦1ã¤ã®WAVã«ä¿å­˜
        
        ä½¿ã„æ–¹:
            texts = ["ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³1", "ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³2", "ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³3"]
            result = tts_engine.generate_continuous_wav(
                texts, 
                "outputs/reaction_part.wav",
                progress_callback=self.update_progress
            )
        
        Args:
            texts: å‡¦ç†ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
            output_path: å‡ºåŠ›WAVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            chunk_size: ãƒ¡ãƒ¢ãƒªç®¡ç†ç”¨ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ100ï¼‰
            resume: ä¸­æ–­ã‹ã‚‰å†é–‹ã™ã‚‹ã‹
            progress_callback: é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
        """
        # é…å»¶åˆæœŸåŒ–
        if self.long_processor is None:
            from core.tts_long_processor import LongTTSProcessor
            self.long_processor = LongTTSProcessor(self)
        
        return self.long_processor.process_texts_to_wav(
            texts_data, output_path, chunk_size, resume, progress_callback
        )