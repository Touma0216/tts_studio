import json
import time
import numpy as np
from typing import Dict, List, Optional, Tuple, Any
from pathlib import Path
from dataclasses import dataclass
from threading import Thread, Event
import traceback
import copy

try:
    import pyopenjtalk
    PYOPENJTALK_AVAILABLE = True
except ImportError:
    PYOPENJTALK_AVAILABLE = False
    print("âš ï¸ pyopenjtalk ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯æ©Ÿèƒ½ã¯åˆ¶é™ã•ã‚Œã¾ã™ã€‚")

@dataclass
class VowelFrame:
    """æ¯éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿"""
    timestamp: float
    vowel: str
    intensity: float
    duration: float
    is_ending: bool = False

@dataclass
class LipSyncData:
    """ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿"""
    text: str
    total_duration: float
    vowel_frames: List[VowelFrame]
    sample_rate: int = 22050

class LipSyncEngine:
    """ãƒ¡ã‚¤ãƒ³ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯ã‚¨ãƒ³ã‚¸ãƒ³ - å®Œå…¨ä¿®æ­£ç‰ˆ
    
    éŸ³å£°ãƒ‡ãƒ¼ã‚¿å¯¾å¿œ + ç¾å®Ÿçš„ãªæ™‚é–“æ¨å®š
    """
    
    def __init__(self):
        self.is_initialized = PYOPENJTALK_AVAILABLE
        self.phoneme_analyzer = None
        self.audio_processor = None
        
        self.ending_protection = {
            'enabled': True,
            'min_duration': 0.15,
            'intensity_boost': 1.05,
            'detection_range': 1
        }
        
        self.settings = {
            'enabled': True,
            'sensitivity': 80,
            'response_speed': 70,
            'mouth_open_scale': 100,
            'auto_optimize': True,
            'delay_compensation': 0,
            'smoothing_factor': 70,
            'prediction_enabled': True,
            'consonant_detection': True,
            'volume_threshold': 5,
            'quality_mode': 'balanced',
            'audio_sync_enabled': True,
            'char_duration': 0.08  # ğŸ”¥ 1æ–‡å­—ã‚ãŸã‚Šã®æ™‚é–“ï¼ˆç§’ï¼‰
        }
        
        self.base_vowel_mapping = {
            'a': {'mouth_open': 100, 'mouth_form': 0},
            'i': {'mouth_open': 30, 'mouth_form': -100},
            'u': {'mouth_open': 40, 'mouth_form': -70},
            'e': {'mouth_open': 60, 'mouth_form': -30},
            'o': {'mouth_open': 80, 'mouth_form': 70},
            'n': {'mouth_open': 10, 'mouth_form': 0},
            'sil': {'mouth_open': 0, 'mouth_form': 0}
        }
        
        self.vowel_mapping = copy.deepcopy(self.base_vowel_mapping)
        self.settings_change_callback = None
        
        self._initialize_modules()
    
    def _initialize_modules(self):
        """é–¢é€£ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®åˆæœŸåŒ–"""
        try:
            if PYOPENJTALK_AVAILABLE:
                test_result = pyopenjtalk.g2p("ãƒ†ã‚¹ãƒˆ")
                if test_result:
                    print("âœ… pyopenjtalkåˆæœŸåŒ–æˆåŠŸ")
                else:
                    print("âš ï¸ pyopenjtalkåˆæœŸåŒ–ã«å•é¡ŒãŒã‚ã‚Šã¾ã™")
                    self.is_initialized = False
            
            from .phoneme_analyzer import PhonemeAnalyzer
            from .audio_realtime_processor import AudioRealtimeProcessor
            
            self.phoneme_analyzer = PhonemeAnalyzer()
            self.audio_processor = AudioRealtimeProcessor()
            
            print("âœ… LipSyncEngineåˆæœŸåŒ–å®Œäº† (å®Œå…¨ä¿®æ­£ç‰ˆ)")
            
        except ImportError as e:
            print(f"âš ï¸ ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            self.is_initialized = False
        except Exception as e:
            print(f"âŒ LipSyncEngineåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            self.is_initialized = False
    
    def is_available(self) -> bool:
        """ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯æ©Ÿèƒ½ãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯"""
        return self.is_initialized and PYOPENJTALK_AVAILABLE
    
    def set_settings_change_callback(self, callback):
        """è¨­å®šå¤‰æ›´é€šçŸ¥ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è¨­å®š"""
        self.settings_change_callback = callback
    
    def update_settings(self, settings: Dict[str, Any]):
        """ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯è¨­å®šã‚’æ›´æ–°"""
        try:
            print(f"ğŸ”§ è¨­å®šæ›´æ–°é–‹å§‹: {list(settings.keys())}")
            
            if 'basic' in settings:
                basic_settings = settings['basic']
                if isinstance(basic_settings, dict):
                    for key in ['enabled', 'sensitivity', 'response_speed', 'mouth_open_scale', 'auto_optimize', 'audio_sync_enabled', 'char_duration']:
                        if key in basic_settings:
                            if key in ['enabled', 'auto_optimize', 'audio_sync_enabled']:
                                self.settings[key] = bool(basic_settings[key])
                            elif key in ['sensitivity', 'response_speed', 'mouth_open_scale']:
                                self.settings[key] = max(0, min(500, int(basic_settings[key])))
                            elif key == 'char_duration':
                                self.settings[key] = max(0.05, min(0.3, float(basic_settings[key])))
                            else:
                                self.settings[key] = basic_settings[key]
            
            if 'ending_protection' in settings:
                protection_settings = settings['ending_protection']
                if isinstance(protection_settings, dict):
                    for key in ['enabled', 'min_duration', 'intensity_boost', 'detection_range']:
                        if key in protection_settings:
                            if key == 'enabled':
                                self.ending_protection[key] = bool(protection_settings[key])
                            elif key == 'min_duration':
                                self.ending_protection[key] = max(0.1, min(1.0, float(protection_settings[key])))
                            elif key == 'intensity_boost':
                                self.ending_protection[key] = max(1.0, min(3.0, float(protection_settings[key])))
                            elif key == 'detection_range':
                                self.ending_protection[key] = max(1, min(10, int(protection_settings[key])))
            
            if 'phoneme' in settings:
                phoneme_settings = settings['phoneme']
                if isinstance(phoneme_settings, dict):
                    for vowel, params in phoneme_settings.items():
                        if vowel in self.vowel_mapping and isinstance(params, dict):
                            if 'mouth_open' in params:
                                try:
                                    value = float(params['mouth_open'])
                                    self.vowel_mapping[vowel]['mouth_open'] = max(0, min(500, value))
                                except (ValueError, TypeError):
                                    pass
                            if 'mouth_form' in params:
                                try:
                                    value = float(params['mouth_form'])
                                    self.vowel_mapping[vowel]['mouth_form'] = max(-500, min(500, value))
                                except (ValueError, TypeError):
                                    pass
            
            if 'advanced' in settings:
                advanced_settings = settings['advanced']
                if isinstance(advanced_settings, dict):
                    for key in ['delay_compensation', 'smoothing_factor', 'prediction_enabled', 
                               'consonant_detection', 'volume_threshold', 'quality_mode']:
                        if key in advanced_settings:
                            value = advanced_settings[key]
                            if key == 'delay_compensation':
                                self.settings[key] = max(-1000, min(1000, int(value)))
                            elif key in ['smoothing_factor', 'volume_threshold']:
                                self.settings[key] = max(0, min(100, int(value)))
                            elif key in ['prediction_enabled', 'consonant_detection']:
                                self.settings[key] = bool(value)
                            else:
                                self.settings[key] = value
            
            if self.settings_change_callback:
                try:
                    self.settings_change_callback(self.get_current_live2d_params())
                except Exception as e:
                    print(f"  âš ï¸ Live2Dé€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")
            
            print(f"âœ… ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯è¨­å®šæ›´æ–°å®Œäº†")
            
        except Exception as e:
            print(f"âŒ è¨­å®šæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()
    
    def get_current_live2d_params(self) -> Dict[str, Any]:
        """Live2Dç”¨ã®ç¾åœ¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—"""
        return {
            'vowel_mapping': copy.deepcopy(self.vowel_mapping),
            'settings': copy.deepcopy(self.settings),
            'ending_protection': copy.deepcopy(self.ending_protection),
            'enabled': self.settings.get('enabled', True)
        }
        
    def analyze_text_for_lipsync(self, text: str, audio_data: np.ndarray = None, 
                                sample_rate: int = None) -> Optional[LipSyncData]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯ç”¨ã«è§£æï¼ˆéŸ³å£°å¯¾å¿œçµ±åˆç‰ˆ + ç„¡éŸ³æ¤œå‡ºï¼‰
        
        Args:
            text: è§£æã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
            audio_data: å®Ÿéš›ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            sample_rate: ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆ
            
        Returns:
            LipSyncData: ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿
        """
        if not self.is_available() or not self.settings['enabled']:
            print("âš ï¸ ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯ç„¡åŠ¹ã¾ãŸã¯ã‚¨ãƒ³ã‚¸ãƒ³åˆ©ç”¨ä¸å¯")
            return None
        
        try:
            # ğŸ”¥ å®Ÿéš›ã®éŸ³å£°é•·ã•ã‚’å–å¾—
            actual_duration = None
            if audio_data is not None and sample_rate is not None:
                actual_duration = len(audio_data) / sample_rate
                print(f"ğŸµ å®Ÿéš›ã®éŸ³å£°é•·ã•: {actual_duration:.3f}ç§’")
            
            print(f"ğŸ” ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯è§£æé–‹å§‹: '{text[:50]}...'")
            
            # pyopenjtalkã§éŸ³ç´ è§£æ
            if not PYOPENJTALK_AVAILABLE:
                return self._fallback_analysis(text, actual_duration)
            
            # éŸ³ç´ ã¨ã‚¿ã‚¤ãƒŸãƒ³ã‚°æƒ…å ±ã‚’å–å¾—
            phoneme_data = self._extract_phonemes_with_timing(text)
            if not phoneme_data:
                return self._fallback_analysis(text, actual_duration)
            
            # èªå°¾éŸ³ç´ ã‚’æ¤œå‡ºãƒ»ãƒãƒ¼ã‚­ãƒ³ã‚°
            phoneme_data = self._mark_ending_phonemes(phoneme_data)
            
            # æ¯éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¤‰æ›
            vowel_frames = self._convert_to_vowel_frames(phoneme_data)
            
            # ğŸ”¥ å®Ÿéš›ã®éŸ³å£°é•·ã•ã«åˆã‚ã›ã¦ã‚¹ã‚±ãƒ¼ãƒ«èª¿æ•´
            if actual_duration is not None:
                estimated_duration = sum(f.duration for f in vowel_frames)
                if estimated_duration > 0:
                    time_scale = actual_duration / estimated_duration
                    print(f"â±ï¸ ã‚¿ã‚¤ãƒ ã‚¹ã‚±ãƒ¼ãƒ«èª¿æ•´: {estimated_duration:.3f}s â†’ {actual_duration:.3f}s (x{time_scale:.3f})")
                    
                    scaled_frames = []
                    current_time = 0.0
                    for frame in vowel_frames:
                        new_duration = frame.duration * time_scale
                        scaled_frame = VowelFrame(
                            timestamp=current_time,
                            vowel=frame.vowel,
                            intensity=frame.intensity,
                            duration=new_duration,
                            is_ending=frame.is_ending
                        )
                        scaled_frames.append(scaled_frame)
                        current_time += new_duration
                    
                    vowel_frames = scaled_frames
                    total_duration = actual_duration
                else:
                    total_duration = actual_duration
            else:
                # ğŸ”¥ éŸ³å£°ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯ã€ãƒ†ã‚­ã‚¹ãƒˆé•·ã‹ã‚‰æ¨å®š
                total_duration = self._estimate_duration_from_text(text)
                estimated_duration = sum(f.duration for f in vowel_frames)
                
                if estimated_duration > 0:
                    time_scale = total_duration / estimated_duration
                    print(f"â±ï¸ ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹èª¿æ•´: {estimated_duration:.3f}s â†’ {total_duration:.3f}s (x{time_scale:.3f})")
                    
                    scaled_frames = []
                    current_time = 0.0
                    for frame in vowel_frames:
                        new_duration = frame.duration * time_scale
                        scaled_frame = VowelFrame(
                            timestamp=current_time,
                            vowel=frame.vowel,
                            intensity=frame.intensity,
                            duration=new_duration,
                            is_ending=frame.is_ending
                        )
                        scaled_frames.append(scaled_frame)
                        current_time += new_duration
                    
                    vowel_frames = scaled_frames
            
            # ğŸ†• ç„¡éŸ³åŒºé–“æ¤œå‡ºã¨é©ç”¨ï¼ˆéŸ³å£°ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã®ã¿ï¼‰
            if audio_data is not None and sample_rate is not None:
                silence_regions = self._detect_silence_regions(
                    audio_data, 
                    sample_rate,
                    min_silence_duration=0.2,  # 0.2ç§’ä»¥ä¸Šã®ç„¡éŸ³ã‚’æ¤œå‡º
                    adaptive_threshold=True
                )
                
                if silence_regions:
                    vowel_frames = self._apply_silence_regions_to_frames(vowel_frames, silence_regions)
            
            # èªå°¾ä¿è­·æ©Ÿèƒ½ã‚’é©ç”¨
            vowel_frames = self._apply_ending_protection_to_frames(vowel_frames)
            
            # è¨­å®šã«å¿œã˜ã¦èª¿æ•´
            vowel_frames = self._apply_settings_to_frames_safe(vowel_frames)
            
            lipsync_data = LipSyncData(
                text=text,
                total_duration=total_duration,
                vowel_frames=vowel_frames
            )
            
            print(f"âœ… ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯è§£æå®Œäº†: {len(vowel_frames)}ãƒ•ãƒ¬ãƒ¼ãƒ , {total_duration:.3f}ç§’")
            
            ending_count = sum(1 for f in vowel_frames if f.is_ending)
            silence_count = sum(1 for f in vowel_frames if f.vowel == 'sil')
            if ending_count > 0:
                print(f"ğŸ›¡ï¸ èªå°¾ä¿è­·é©ç”¨: {ending_count}å€‹ã®èªå°¾éŸ³ç´ ")
            if silence_count > 0:
                print(f"ğŸ”‡ ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ : {silence_count}å€‹")
            
            return lipsync_data
            
        except Exception as e:
            print(f"âŒ ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯è§£æã‚¨ãƒ©ãƒ¼: {e}")
            print(traceback.format_exc())
            return self._fallback_analysis(text, actual_duration)
    
    def _estimate_duration_from_text(self, text: str) -> float:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç·æ™‚é–“ã‚’æ¨å®šï¼ˆç¾å®Ÿçš„ãªå€¤ï¼‰"""
        # æ—¥æœ¬èªã®å¹³å‡çš„ãªèª­ã¿ä¸Šã’é€Ÿåº¦ã‚’è€ƒæ…®
        char_count = len(text)
        char_duration = self.settings.get('char_duration', 0.08)  # 1æ–‡å­—0.08ç§’
        
        # å¥èª­ç‚¹ã‚„è¨˜å·ã¯æ™‚é–“ã«å«ã‚ãªã„
        punctuation_count = sum(1 for c in text if c in 'ã€‚ã€ï¼ï¼Ÿâ€¦ï½ãƒ¼ãƒ»')
        effective_chars = max(1, char_count - punctuation_count * 0.5)
        
        estimated = effective_chars * char_duration
        print(f"ğŸ“ æ¨å®šæ™‚é–“: {char_count}æ–‡å­— â†’ {estimated:.3f}ç§’ ({char_duration}ç§’/æ–‡å­—)")
        
        return max(0.5, estimated)
    
    def _extract_phonemes_with_timing(self, text: str) -> Optional[List[Dict]]:
        """pyopenjtalkã§éŸ³ç´ ã¨ã‚¿ã‚¤ãƒŸãƒ³ã‚°æƒ…å ±ã‚’æŠ½å‡º"""
        try:
            print(f"ğŸ” éŸ³ç´ æŠ½å‡ºé–‹å§‹: '{text}'")
            
            phonemes_result = pyopenjtalk.g2p(text, kana=False)
            print(f"  ğŸ“ g2pçµæœ: '{phonemes_result}'")
            
            if phonemes_result and phonemes_result.strip():
                raw_phoneme_sequence = phonemes_result.strip().split()
                print(f"  ğŸ”— ç”ŸéŸ³ç´ åˆ—: {raw_phoneme_sequence}")
                
                phoneme_sequence = self._merge_phonemes(raw_phoneme_sequence)
                print(f"  ğŸ”— çµåˆå¾ŒéŸ³ç´ åˆ—: {phoneme_sequence}")
                
                phoneme_list = []
                current_time = 0.0
                
                for i, phoneme in enumerate(phoneme_sequence):
                    if not phoneme or phoneme.isspace():
                        continue
                    
                    # ğŸ”¥ çŸ­ã„åŸºæœ¬æ™‚é–“ï¼ˆå¾Œã§ã‚¹ã‚±ãƒ¼ãƒ«èª¿æ•´ã•ã‚Œã‚‹ï¼‰
                    duration = self._estimate_phoneme_duration_base(phoneme)
                    
                    phoneme_data = {
                        'phoneme': phoneme,
                        'start_time': current_time,
                        'duration': duration,
                        'is_ending': False,
                        'index': i
                    }
                    
                    phoneme_list.append(phoneme_data)
                    current_time += duration
                    print(f"    {i}: {phoneme} ({duration:.2f}s)")
                
                print(f"  âœ… éŸ³ç´ æŠ½å‡ºæˆåŠŸ: {len(phoneme_list)}å€‹")
                return phoneme_list
            
            return None
            
        except Exception as e:
            print(f"âŒ éŸ³ç´ æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()
            return None
    
    def _merge_phonemes(self, raw_phonemes: List[str]) -> List[str]:
        """åˆ†è§£ã•ã‚Œã™ããŸéŸ³ç´ ã‚’é©åˆ‡ã«çµåˆã™ã‚‹"""
        if not raw_phonemes:
            return []
        
        merged = []
        i = 0
        
        while i < len(raw_phonemes):
            current = raw_phonemes[i]
            
            if i + 1 < len(raw_phonemes):
                next_phoneme = raw_phonemes[i + 1]
                
                combined = self._try_combine_phonemes(current, next_phoneme)
                if combined:
                    print(f"    ğŸ”— éŸ³ç´ çµåˆ: {current} + {next_phoneme} â†’ {combined}")
                    merged.append(combined)
                    i += 2
                    continue
            
            converted = self._convert_single_phoneme(current)
            merged.append(converted)
            if converted != current:
                print(f"    ğŸ”„ éŸ³ç´ å¤‰æ›: {current} â†’ {converted}")
            i += 1
        
        return merged
    
    def _try_combine_phonemes(self, first: str, second: str) -> Optional[str]:
        """2ã¤ã®éŸ³ç´ ã‚’çµåˆã§ãã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        combination_rules = {
            ('k', 'a'): 'ka', ('k', 'i'): 'ki', ('k', 'u'): 'ku', ('k', 'e'): 'ke', ('k', 'o'): 'ko',
            ('g', 'a'): 'ga', ('g', 'i'): 'gi', ('g', 'u'): 'gu', ('g', 'e'): 'ge', ('g', 'o'): 'go',
            ('s', 'a'): 'sa', ('s', 'i'): 'si', ('s', 'u'): 'su', ('s', 'e'): 'se', ('s', 'o'): 'so',
            ('z', 'a'): 'za', ('z', 'i'): 'zi', ('z', 'u'): 'zu', ('z', 'e'): 'ze', ('z', 'o'): 'zo',
            ('t', 'a'): 'ta', ('t', 'i'): 'ti', ('t', 'u'): 'tu', ('t', 'e'): 'te', ('t', 'o'): 'to',
            ('d', 'a'): 'da', ('d', 'i'): 'di', ('d', 'u'): 'du', ('d', 'e'): 'de', ('d', 'o'): 'do',
            ('n', 'a'): 'na', ('n', 'i'): 'ni', ('n', 'u'): 'nu', ('n', 'e'): 'ne', ('n', 'o'): 'no',
            ('h', 'a'): 'ha', ('h', 'i'): 'hi', ('h', 'u'): 'hu', ('h', 'e'): 'he', ('h', 'o'): 'ho',
            ('b', 'a'): 'ba', ('b', 'i'): 'bi', ('b', 'u'): 'bu', ('b', 'e'): 'be', ('b', 'o'): 'bo',
            ('p', 'a'): 'pa', ('p', 'i'): 'pi', ('p', 'u'): 'pu', ('p', 'e'): 'pe', ('p', 'o'): 'po',
            ('m', 'a'): 'ma', ('m', 'i'): 'mi', ('m', 'u'): 'mu', ('m', 'e'): 'me', ('m', 'o'): 'mo',
            ('y', 'a'): 'ya', ('y', 'u'): 'yu', ('y', 'o'): 'yo',
            ('r', 'a'): 'ra', ('r', 'i'): 'ri', ('r', 'u'): 'ru', ('r', 'e'): 're', ('r', 'o'): 'ro',
            ('w', 'a'): 'wa', ('w', 'i'): 'wi', ('w', 'u'): 'wu', ('w', 'e'): 'we', ('w', 'o'): 'wo',
            ('ch', 'i'): 'chi', ('ch', 'u'): 'chu', ('ch', 'o'): 'cho',
            ('sh', 'i'): 'shi', ('sh', 'u'): 'shu', ('sh', 'o'): 'sho',
            ('j', 'i'): 'ji', ('j', 'u'): 'ju', ('j', 'o'): 'jo',
            ('w', 'a'): 'ha',
        }
        
        key = (first, second)
        return combination_rules.get(key)
    
    def _convert_single_phoneme(self, phoneme: str) -> str:
        """å˜ç‹¬éŸ³ç´ ã®å¤‰æ›ãƒ«ãƒ¼ãƒ«"""
        single_conversions = {
            'w': 'wa',
            'n': 'N',
        }
        
        return single_conversions.get(phoneme, phoneme)
    
    def _mark_ending_phonemes(self, phoneme_data: List[Dict]) -> List[Dict]:
        """èªå°¾éŸ³ç´ ã‚’ãƒãƒ¼ã‚­ãƒ³ã‚°"""
        if not self.ending_protection['enabled'] or not phoneme_data:
            return phoneme_data
        
        meaningful_phonemes = []
        for i, data in enumerate(phoneme_data):
            phoneme = data['phoneme']
            if phoneme not in ['pau', 'sil', 'sp', 'Q', 'cl']:
                meaningful_phonemes.append((i, phoneme))
        
        if not meaningful_phonemes:
            return phoneme_data
        
        ending_count = 1
        
        print(f"ğŸ›¡ï¸ èªå°¾æ¤œå‡º: æœ‰æ„å‘³éŸ³ç´ {len(meaningful_phonemes)}å€‹ä¸­ã€æœ«å°¾{ending_count}å€‹ã‚’èªå°¾ã¨ã—ã¦è¨­å®š")
        
        for j in range(ending_count):
            if j < len(meaningful_phonemes):
                idx, phoneme = meaningful_phonemes[-(j + 1)]
                phoneme_data[idx]['is_ending'] = True
                print(f"  ğŸ›¡ï¸ èªå°¾éŸ³ç´ : {phoneme}")
        
        return phoneme_data
    
    def _estimate_phoneme_duration_base(self, phoneme: str) -> float:
        """éŸ³ç´ ã®åŸºæœ¬æ¨å®šå†ç”Ÿæ™‚é–“ã‚’è¨ˆç®—ï¼ˆçŸ­ç¸®ç‰ˆãƒ»å¾Œã§ã‚¹ã‚±ãƒ¼ãƒ«èª¿æ•´ï¼‰"""
        # ğŸ”¥ å…¨ä½“çš„ã«çŸ­ãè¨­å®šï¼ˆå¾Œã§ç·æ™‚é–“ã«åˆã‚ã›ã¦ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
        if phoneme in ['a', 'i', 'u', 'e', 'o']:
            return 1.0  # ç›¸å¯¾å€¤ï¼ˆæ¯éŸ³ï¼‰
        elif phoneme in ['N', 'Q', 'pau', 'cl']:
            return 0.3  # ç›¸å¯¾å€¤ï¼ˆç„¡éŸ³ç³»ã¯çŸ­ãï¼‰
        else:
            return 0.7  # ç›¸å¯¾å€¤ï¼ˆå­éŸ³ï¼‰
    
    def _convert_to_vowel_frames(self, phoneme_data: List[Dict]) -> List[VowelFrame]:
        """éŸ³ç´ ãƒ‡ãƒ¼ã‚¿ã‚’æ¯éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¤‰æ›"""
        vowel_frames = []
        
        print(f"  ğŸ”— éŸ³ç´ â†’æ¯éŸ³å¤‰æ›é–‹å§‹: {len(phoneme_data)}å€‹ã®éŸ³ç´ ")
        
        for i, phoneme_info in enumerate(phoneme_data):
            phoneme = phoneme_info['phoneme']
            start_time = phoneme_info['start_time']
            duration = phoneme_info['duration']
            is_ending = phoneme_info.get('is_ending', False)
            
            vowel = self._map_phoneme_to_vowel_fixed(phoneme)
            intensity = self._calculate_intensity(phoneme, vowel)
            
            vowel_frame = VowelFrame(
                timestamp=start_time,
                vowel=vowel,
                intensity=intensity,
                duration=duration,
                is_ending=is_ending
            )
            
            vowel_frames.append(vowel_frame)
            
            ending_mark = "ğŸ›¡ï¸" if is_ending else ""
            print(f"    {i}: {phoneme} â†’ {vowel} (å¼·åº¦:{intensity:.2f}, æ™‚é–“:{duration:.2f}s) {ending_mark}")
        
        print(f"  âœ… æ¯éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ å¤‰æ›å®Œäº†: {len(vowel_frames)}å€‹")
        return vowel_frames
    
    def _map_phoneme_to_vowel_fixed(self, phoneme: str) -> str:
        """éŸ³ç´ ã‚’æ¯éŸ³ã«ãƒãƒƒãƒ”ãƒ³ã‚°"""
        complete_vowel_map = {
            'a': 'a', 'i': 'i', 'u': 'u', 'e': 'e', 'o': 'o',
            'A': 'a', 'I': 'i', 'U': 'u', 'E': 'e', 'O': 'o',
            'ka': 'a', 'ga': 'a', 'sa': 'a', 'za': 'a', 'ta': 'a', 'da': 'a',
            'na': 'a', 'ha': 'a', 'ba': 'a', 'pa': 'a', 'ma': 'a', 'ya': 'a',
            'ra': 'a', 'wa': 'a', 'kya': 'a', 'gya': 'a', 'sha': 'a', 'ja': 'a',
            'cha': 'a', 'nya': 'a', 'hya': 'a', 'bya': 'a', 'pya': 'a', 'mya': 'a', 'rya': 'a',
            'ki': 'i', 'gi': 'i', 'si': 'i', 'zi': 'i', 'ti': 'i', 'di': 'i',
            'ni': 'i', 'hi': 'i', 'bi': 'i', 'pi': 'i', 'mi': 'i', 'ri': 'i',
            'chi': 'i', 'ji': 'i', 'shi': 'i',
            'ku': 'u', 'gu': 'u', 'su': 'u', 'zu': 'u', 'tu': 'u', 'du': 'u',
            'nu': 'u', 'hu': 'u', 'bu': 'u', 'pu': 'u', 'mu': 'u', 'yu': 'u',
            'ru': 'u', 'tsu': 'u', 'kyu': 'u', 'gyu': 'u', 'shu': 'u', 'ju': 'u',
            'chu': 'u', 'nyu': 'u', 'hyu': 'u', 'byu': 'u', 'pyu': 'u', 'myu': 'u', 'ryu': 'u',
            'ke': 'e', 'ge': 'e', 'se': 'e', 'ze': 'e', 'te': 'e', 'de': 'e',
            'ne': 'e', 'he': 'e', 'be': 'e', 'pe': 'e', 'me': 'e', 're': 'e',
            'ko': 'o', 'go': 'o', 'so': 'o', 'zo': 'o', 'to': 'o', 'do': 'o',
            'no': 'o', 'ho': 'o', 'bo': 'o', 'po': 'o', 'mo': 'o', 'yo': 'o',
            'ro': 'o', 'kyo': 'o', 'gyo': 'o', 'sho': 'o', 'jo': 'o', 'cho': 'o',
            'nyo': 'o', 'hyo': 'o', 'byo': 'o', 'pyo': 'o', 'myo': 'o', 'ryo': 'o',
            'N': 'n',
            'Q': 'sil',
            'pau': 'sil',
            'sil': 'sil',
            'sp': 'sil',
            'cl': 'sil',  # ğŸ”¥ clã‚‚ç„¡éŸ³ã¨ã—ã¦æ‰±ã†
            'ãƒ¼': 'a',
            'w': 'u',
            'v': 'u',
            'f': 'u',
            'sh': 'i',  # ğŸ”¥ shã‚’è¿½åŠ 
        }
        
        if phoneme in complete_vowel_map:
            mapped_vowel = complete_vowel_map[phoneme]
            if phoneme not in ['pau', 'sil', 'sp', 'Q', 'cl'] and len(phoneme) > 1:
                print(f"      ğŸ”— {phoneme} â†’ {mapped_vowel}")
            return mapped_vowel
        
        if phoneme.endswith('a'):
            return 'a'
        elif phoneme.endswith('i'):
            return 'i'
        elif phoneme.endswith('u'):
            return 'u'
        elif phoneme.endswith('e'):
            return 'e'
        elif phoneme.endswith('o'):
            return 'o'
        
        print(f"      âš ï¸ æœªçŸ¥éŸ³ç´ : {phoneme} â†’ sil")
        return 'sil'  # ğŸ”¥ æœªçŸ¥éŸ³ç´ ã¯ç„¡éŸ³ã«ã™ã‚‹
    
    def _apply_ending_protection_to_frames(self, vowel_frames: List[VowelFrame]) -> List[VowelFrame]:
        """èªå°¾ä¿è­·æ©Ÿèƒ½ã‚’ãƒ•ãƒ¬ãƒ¼ãƒ ã«é©ç”¨"""
        if not self.ending_protection['enabled']:
            return vowel_frames
        
        protected_frames = []
        
        for frame in vowel_frames:
            if frame.is_ending and frame.vowel != 'sil':
                protected_intensity = min(1.0, frame.intensity * self.ending_protection['intensity_boost'])
                
                protected_frame = VowelFrame(
                    timestamp=frame.timestamp,
                    vowel=frame.vowel,
                    intensity=protected_intensity,
                    duration=frame.duration,
                    is_ending=True
                )
                
                if protected_intensity > frame.intensity:
                    print(f"  ğŸ›¡ï¸ èªå°¾ä¿è­·: {frame.vowel} (å¼·åº¦:{frame.intensity:.2f}â†’{protected_intensity:.2f})")
            else:
                protected_frame = frame
            
            protected_frames.append(protected_frame)
        
        return protected_frames
    
    def _calculate_intensity(self, phoneme: str, vowel: str) -> float:
        """éŸ³ç´ ã®å¼·åº¦ã‚’è¨ˆç®—"""
        if vowel in ['a', 'e', 'o']:
            return 0.9
        elif vowel in ['i', 'u']:
            return 0.7
        elif vowel == 'n':
            return 0.3
        else:
            return 0.1
    
    def _apply_settings_to_frames_safe(self, frames: List[VowelFrame]) -> List[VowelFrame]:
        """è¨­å®šã«åŸºã¥ã„ã¦ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’èª¿æ•´"""
        adjusted_frames = []
        sensitivity = self.settings['sensitivity'] / 100.0
        
        print(f"ğŸ”§ ãƒ•ãƒ¬ãƒ¼ãƒ èª¿æ•´: æ„Ÿåº¦={sensitivity:.2f}, ãƒ•ãƒ¬ãƒ¼ãƒ æ•°={len(frames)}")
        
        for frame in frames:
            adjusted_intensity = frame.intensity * sensitivity
            adjusted_intensity = max(0.0, min(1.0, adjusted_intensity))
            
            adjusted_frame = VowelFrame(
                timestamp=frame.timestamp,
                vowel=frame.vowel,
                intensity=adjusted_intensity,
                duration=frame.duration,
                is_ending=frame.is_ending
            )
            
            adjusted_frames.append(adjusted_frame)
        
        return adjusted_frames
    
    def _fallback_analysis(self, text: str, actual_duration: float = None) -> LipSyncData:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è§£æ"""
        print("âš ï¸ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã§ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯è§£æ")
        
        if actual_duration is None:
            actual_duration = self._estimate_duration_from_text(text)
        
        char_count = len(text)
        duration_per_char = actual_duration / max(char_count, 1)
        
        vowel_frames = []
        default_vowels = ['a', 'i', 'u', 'e', 'o']
        
        for i in range(min(char_count, 10)):
            vowel = default_vowels[i % len(default_vowels)]
            is_ending = i >= char_count - 1
            
            frame = VowelFrame(
                timestamp=i * duration_per_char,
                vowel=vowel,
                intensity=0.8 if is_ending else 0.7,
                duration=duration_per_char,
                is_ending=is_ending
            )
            vowel_frames.append(frame)
        
        return LipSyncData(
            text=text,
            total_duration=actual_duration,
            vowel_frames=vowel_frames
        )
    
    def generate_lipsync_keyframes(self, lipsync_data: LipSyncData, fps: int = 30) -> Dict[str, Any]:
        """Live2Dç”¨ã®ã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
        try:
            if not lipsync_data or not lipsync_data.vowel_frames:
                return {}
            
            total_frames = int(lipsync_data.total_duration * fps)
            keyframes = {
                'total_duration': lipsync_data.total_duration,
                'fps': fps,
                'total_frames': total_frames,
                'vowel_keyframes': {},
                'mouth_params': {},
                'ending_protection_applied': self.ending_protection['enabled']
            }
            
            for vowel in ['a', 'i', 'u', 'e', 'o', 'n']:
                keyframes['vowel_keyframes'][vowel] = []
            
            keyframes['mouth_params'] = {
                'mouth_open': [],
                'mouth_form': []
            }
            
            mouth_scale = self.settings.get('mouth_open_scale', 100) / 100.0
            
            for frame_num in range(total_frames):
                current_time = frame_num / fps
                
                active_frame = self._find_active_vowel_frame(lipsync_data.vowel_frames, current_time)
                
                if active_frame:
                    vowel = active_frame.vowel
                    intensity = active_frame.intensity
                    
                    for v in keyframes['vowel_keyframes']:
                        value = intensity if v == vowel else 0.0
                        keyframes['vowel_keyframes'][v].append({
                            'frame': frame_num,
                            'value': value,
                            'is_ending': active_frame.is_ending
                        })
                    
                    if vowel in self.vowel_mapping:
                        mapping = self.vowel_mapping[vowel]
                        
                        base_mouth_open = mapping['mouth_open']
                        scaled_mouth_open = base_mouth_open * mouth_scale
                        mouth_open = (scaled_mouth_open / 100.0) * intensity
                        
                        mouth_form = (mapping['mouth_form'] / 100.0) * intensity
                    else:
                        mouth_open = 0.0
                        mouth_form = 0.0
                    
                    keyframes['mouth_params']['mouth_open'].append({
                        'frame': frame_num,
                        'value': mouth_open,
                        'is_ending': active_frame.is_ending
                    })
                    keyframes['mouth_params']['mouth_form'].append({
                        'frame': frame_num,
                        'value': mouth_form,
                        'is_ending': active_frame.is_ending
                    })
                else:
                    for v in keyframes['vowel_keyframes']:
                        keyframes['vowel_keyframes'][v].append({
                            'frame': frame_num,
                            'value': 0.0,
                            'is_ending': False
                        })
                    
                    keyframes['mouth_params']['mouth_open'].append({
                        'frame': frame_num,
                        'value': 0.0,
                        'is_ending': False
                    })
                    keyframes['mouth_params']['mouth_form'].append({
                        'frame': frame_num,
                        'value': 0.0,
                        'is_ending': False
                    })
            
            ending_frame_count = sum(1 for frames in keyframes['vowel_keyframes'].values() 
                                   for frame in frames if frame.get('is_ending', False))
            
            print(f"âœ… ã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ãƒ ç”Ÿæˆå®Œäº†: {total_frames}ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆèªå°¾ãƒ•ãƒ¬ãƒ¼ãƒ : {ending_frame_count}ï¼‰")
            return keyframes
            
        except Exception as e:
            print(f"âŒ ã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ãƒ ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()
            return {}
    
    def _find_active_vowel_frame(self, vowel_frames: List[VowelFrame], current_time: float) -> Optional[VowelFrame]:
        """æŒ‡å®šæ™‚é–“ã«ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªæ¯éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è¦‹ã¤ã‘ã‚‹"""
        for frame in vowel_frames:
            if frame.timestamp <= current_time < (frame.timestamp + frame.duration):
                return frame
        return None
    
    def export_lipsync_data(self, lipsync_data: LipSyncData, output_path: str) -> bool:
        """ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›"""
        try:
            keyframes = self.generate_lipsync_keyframes(lipsync_data)
            
            export_data = {
                'metadata': {
                    'text': lipsync_data.text,
                    'total_duration': lipsync_data.total_duration,
                    'frame_count': len(lipsync_data.vowel_frames),
                    'generated_at': time.time(),
                    'engine_version': '2.3.0',
                    'ending_protection_enabled': self.ending_protection['enabled']
                },
                'settings': self.settings,
                'ending_protection': self.ending_protection,
                'vowel_mapping': self.vowel_mapping,
                'keyframes': keyframes,
                'raw_vowel_frames': [
                    {
                        'timestamp': frame.timestamp,
                        'vowel': frame.vowel,
                        'intensity': frame.intensity,
                        'duration': frame.duration,
                        'is_ending': frame.is_ending
                    }
                    for frame in lipsync_data.vowel_frames
                ]
            }
            
            Path(output_path).parent.mkdir(parents=True, exist_ok=True)
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿å‡ºåŠ›å®Œäº†: {output_path}")
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿å‡ºåŠ›ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def get_vowel_mapping(self) -> Dict[str, Dict[str, float]]:
        """ç¾åœ¨ã®æ¯éŸ³ãƒãƒƒãƒ”ãƒ³ã‚°è¨­å®šã‚’å–å¾—"""
        return copy.deepcopy(self.vowel_mapping)
    
    def get_settings(self) -> Dict[str, Any]:
        """ç¾åœ¨ã®ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯è¨­å®šã‚’å–å¾—"""
        return copy.deepcopy(self.settings)
    
    def get_ending_protection_settings(self) -> Dict[str, Any]:
        """èªå°¾ä¿è­·è¨­å®šã‚’å–å¾—"""
        return copy.deepcopy(self.ending_protection)
    
    def reset_vowel_mapping(self):
        """æ¯éŸ³ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«ãƒªã‚»ãƒƒãƒˆ"""
        self.vowel_mapping = copy.deepcopy(self.base_vowel_mapping)
        print("âœ… æ¯éŸ³ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸ")
    
    def debug_analysis(self, text: str) -> Dict[str, Any]:
        """ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šè§£æçµæœã®è©³ç´°æƒ…å ±"""
        debug_info = {
            'input_text': text,
            'pyopenjtalk_available': PYOPENJTALK_AVAILABLE,
            'engine_initialized': self.is_initialized,
            'settings': self.settings,
            'ending_protection': self.ending_protection,
            'vowel_mapping': self.vowel_mapping,
            'base_vowel_mapping': self.base_vowel_mapping
        }
        
        if PYOPENJTALK_AVAILABLE:
            try:
                g2p_result = pyopenjtalk.g2p(text, kana=False)
                debug_info['g2p_result'] = g2p_result
                
                lipsync_data = self.analyze_text_for_lipsync(text)
                if lipsync_data:
                    ending_count = sum(1 for f in lipsync_data.vowel_frames if f.is_ending)
                    debug_info['analysis_result'] = {
                        'total_duration': lipsync_data.total_duration,
                        'frame_count': len(lipsync_data.vowel_frames),
                        'ending_frame_count': ending_count,
                        'vowel_sequence': [f.vowel for f in lipsync_data.vowel_frames],
                        'ending_frames': [f.vowel for f in lipsync_data.vowel_frames if f.is_ending]
                    }
                
            except Exception as e:
                debug_info['error'] = str(e)
        
        return debug_info
    
    def _detect_silence_regions(self, audio_data: np.ndarray, sample_rate: int, 
                            min_silence_duration: float = 0.2,
                            adaptive_threshold: bool = True) -> List[Tuple[float, float]]:
        """WAVãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç„¡éŸ³åŒºé–“ã‚’æ¤œå‡ºï¼ˆé©å¿œçš„é–¾å€¤ï¼‰
        
        Args:
            audio_data: éŸ³å£°ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ¢ãƒãƒ©ãƒ«ã€float32ï¼‰
            sample_rate: ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆ
            min_silence_duration: ç„¡éŸ³ã¨ã—ã¦æ‰±ã†æœ€å°æ™‚é–“ï¼ˆç§’ï¼‰
            adaptive_threshold: é©å¿œçš„é–¾å€¤ã‚’ä½¿ç”¨ã™ã‚‹ã‹
            
        Returns:
            ç„¡éŸ³åŒºé–“ã®ãƒªã‚¹ãƒˆ [(é–‹å§‹æ™‚é–“, çµ‚äº†æ™‚é–“), ...]
        """
        try:
            print(f"ğŸ”‡ ç„¡éŸ³åŒºé–“æ¤œå‡ºé–‹å§‹: {len(audio_data)/sample_rate:.2f}ç§’ã®éŸ³å£°")
            
            # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†
            if audio_data.ndim > 1:
                # ã‚¹ãƒ†ãƒ¬ã‚ªã®å ´åˆã¯ãƒ¢ãƒãƒ©ãƒ«ã«å¤‰æ›
                audio_data = np.mean(audio_data, axis=1)
            
            # ãƒ•ãƒ¬ãƒ¼ãƒ é•·ï¼ˆ50ms = 0.05ç§’ï¼‰
            frame_length = int(sample_rate * 0.05)
            hop_length = frame_length // 2  # 50%ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—
            
            # RMSï¼ˆäºŒä¹—å¹³å‡å¹³æ–¹æ ¹ï¼‰ã‚’è¨ˆç®—
            num_frames = (len(audio_data) - frame_length) // hop_length + 1
            rms_values = np.zeros(num_frames)
            
            for i in range(num_frames):
                start_idx = i * hop_length
                end_idx = start_idx + frame_length
                frame = audio_data[start_idx:end_idx]
                rms_values[i] = np.sqrt(np.mean(frame ** 2))
            
            # ğŸ”¥ é©å¿œçš„é–¾å€¤ã®è¨ˆç®—
            if adaptive_threshold:
                # éŸ³é‡åˆ†å¸ƒã‚’è§£æ
                non_zero_rms = rms_values[rms_values > 1e-6]  # ã»ã¼0ã®å€¤ã¯é™¤å¤–
                
                if len(non_zero_rms) > 0:
                    avg_rms = np.mean(non_zero_rms)
                    std_rms = np.std(non_zero_rms)
                    percentile_5 = np.percentile(non_zero_rms, 5)  # ä¸‹ä½5%
                    
                    # é–¾å€¤ = å¹³å‡ã®15% ã¨ 5ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ« ã®å¤§ãã„æ–¹
                    threshold = max(avg_rms * 0.15, percentile_5)
                    
                    print(f"  ğŸ“Š éŸ³é‡çµ±è¨ˆ: å¹³å‡={avg_rms:.6f}, æ¨™æº–åå·®={std_rms:.6f}")
                    print(f"  ğŸ“Š ä¸‹ä½5%={percentile_5:.6f}")
                    print(f"  ğŸ¯ é©å¿œçš„é–¾å€¤={threshold:.6f}")
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    threshold = 0.01
                    print(f"  âš ï¸ éŸ³é‡ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã€å›ºå®šé–¾å€¤ä½¿ç”¨={threshold}")
            else:
                # å›ºå®šé–¾å€¤
                threshold = self.settings.get('silence_threshold', 0.01)
                print(f"  ğŸ¯ å›ºå®šé–¾å€¤={threshold}")
            
            # ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æ¤œå‡ºï¼ˆé–¾å€¤ä»¥ä¸‹ï¼‰
            is_silence = rms_values < threshold
            
            # ãƒ•ãƒ¬ãƒ¼ãƒ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ™‚é–“ã«å¤‰æ›
            frame_times = np.arange(num_frames) * hop_length / sample_rate
            
            # é€£ç¶šã™ã‚‹ç„¡éŸ³åŒºé–“ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
            silence_regions = []
            in_silence = False
            silence_start = 0.0
            
            for i in range(len(is_silence)):
                if is_silence[i] and not in_silence:
                    # ç„¡éŸ³é–‹å§‹
                    in_silence = True
                    silence_start = frame_times[i]
                elif not is_silence[i] and in_silence:
                    # ç„¡éŸ³çµ‚äº†
                    in_silence = False
                    silence_end = frame_times[i]
                    duration = silence_end - silence_start
                    
                    # æœ€å°æ™‚é–“ä»¥ä¸Šã®ç„¡éŸ³ã®ã¿è¨˜éŒ²
                    if duration >= min_silence_duration:
                        silence_regions.append((silence_start, silence_end))
                        print(f"    ğŸ”‡ ç„¡éŸ³åŒºé–“: {silence_start:.2f}s - {silence_end:.2f}s ({duration:.2f}s)")
            
            # æœ€å¾Œã¾ã§ç„¡éŸ³ã ã£ãŸå ´åˆ
            if in_silence:
                silence_end = frame_times[-1]
                duration = silence_end - silence_start
                if duration >= min_silence_duration:
                    silence_regions.append((silence_start, silence_end))
                    print(f"    ğŸ”‡ ç„¡éŸ³åŒºé–“: {silence_start:.2f}s - {silence_end:.2f}s ({duration:.2f}s)")
            
            print(f"âœ… ç„¡éŸ³åŒºé–“æ¤œå‡ºå®Œäº†: {len(silence_regions)}å€‹ã®ç„¡éŸ³åŒºé–“")
            
            return silence_regions
            
        except Exception as e:
            print(f"âŒ ç„¡éŸ³æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()
            return []


    def _apply_silence_regions_to_frames(self, vowel_frames: List[VowelFrame], 
                                        silence_regions: List[Tuple[float, float]]) -> List[VowelFrame]:
        """ç„¡éŸ³åŒºé–“ã‚’æ¯éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ã«é©ç”¨
        
        Args:
            vowel_frames: å…ƒã®æ¯éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ãƒªã‚¹ãƒˆ
            silence_regions: ç„¡éŸ³åŒºé–“ã®ãƒªã‚¹ãƒˆ [(é–‹å§‹, çµ‚äº†), ...]
            
        Returns:
            ç„¡éŸ³åŒºé–“ãŒé©ç”¨ã•ã‚ŒãŸæ¯éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ãƒªã‚¹ãƒˆ
        """
        if not silence_regions:
            return vowel_frames
        
        print(f"ğŸ”‡ ç„¡éŸ³åŒºé–“é©ç”¨é–‹å§‹: {len(vowel_frames)}ãƒ•ãƒ¬ãƒ¼ãƒ , {len(silence_regions)}ç„¡éŸ³åŒºé–“")
        
        corrected_frames = []
        correction_count = 0
        
        for frame in vowel_frames:
            frame_start = frame.timestamp
            frame_end = frame.timestamp + frame.duration
            frame_mid = (frame_start + frame_end) / 2
            
            # ã“ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãŒç„¡éŸ³åŒºé–“å†…ã«ã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            is_in_silence = False
            for silence_start, silence_end in silence_regions:
                # ãƒ•ãƒ¬ãƒ¼ãƒ ã®ä¸­å¿ƒãŒç„¡éŸ³åŒºé–“å†…ã«ã‚ã‚‹ã‹
                if silence_start <= frame_mid <= silence_end:
                    is_in_silence = True
                    break
            
            if is_in_silence and frame.vowel != 'sil':
                # ç„¡éŸ³åŒºé–“å†… â†’ å¼·åˆ¶çš„ã«silã«å¤‰æ›
                corrected_frame = VowelFrame(
                    timestamp=frame.timestamp,
                    vowel='sil',
                    intensity=0.0,
                    duration=frame.duration,
                    is_ending=False  # ç„¡éŸ³ãªã®ã§èªå°¾ãƒ•ãƒ©ã‚°ã‚‚è§£é™¤
                )
                correction_count += 1
                print(f"  ğŸ”‡ ä¿®æ­£: [{frame_start:.2f}s] {frame.vowel} â†’ sil")
            else:
                # ç„¡éŸ³åŒºé–“å¤– â†’ ãã®ã¾ã¾
                corrected_frame = frame
            
            corrected_frames.append(corrected_frame)
        
        print(f"âœ… ç„¡éŸ³åŒºé–“é©ç”¨å®Œäº†: {correction_count}ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä¿®æ­£")
        
        return corrected_frames

    def analyze_long_wav_for_lipsync(self, wav_path: str, text: str = None,
                                    min_segment_gap: float = 1.0) -> Optional[LipSyncData]:
        """é•·æ™‚é–“WAVç”¨ã®ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯ç”Ÿæˆï¼ˆã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†å‰²æ–¹å¼ï¼‰
        
        Args:
            wav_path: WAVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            text: ãƒ†ã‚­ã‚¹ãƒˆï¼ˆNoneã®å ´åˆã¯Whisperã§è‡ªå‹•æ–‡å­—èµ·ã“ã—ï¼‰
            min_segment_gap: ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†å‰²ã®æœ€å°ç„¡éŸ³æ™‚é–“ï¼ˆç§’ï¼‰
            
        Returns:
            LipSyncData: çµåˆã•ã‚ŒãŸãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿
        """
        try:
            import soundfile as sf
            from pathlib import Path
            
            print(f"ğŸ¬ é•·æ™‚é–“WAVè§£æé–‹å§‹: {Path(wav_path).name}")
            
            # WAVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            audio_data, sample_rate = sf.read(wav_path, dtype='float32')
            total_duration = len(audio_data) / sample_rate
            print(f"ğŸ“Š ç·æ™‚é–“: {total_duration:.2f}ç§’ ({total_duration/60:.1f}åˆ†)")
            
            # ã‚¹ãƒ†ãƒ¬ã‚ªâ†’ãƒ¢ãƒãƒ©ãƒ«å¤‰æ›
            if audio_data.ndim > 1:
                audio_data = np.mean(audio_data, axis=1)
            
            # ã‚¹ãƒ†ãƒƒãƒ—1: Whisperã§æ–‡å­—èµ·ã“ã— + ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå–å¾—
            segments = self._get_whisper_segments(wav_path, text)
            if not segments:
                print("âš ï¸ Whisperã‚»ã‚°ãƒ¡ãƒ³ãƒˆå–å¾—å¤±æ•—ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†")
                return self._fallback_long_wav_analysis(audio_data, sample_rate, text)
            
            print(f"âœ… Whisperã‚»ã‚°ãƒ¡ãƒ³ãƒˆå–å¾—: {len(segments)}å€‹")
            
            # ã‚¹ãƒ†ãƒƒãƒ—2: ç„¡éŸ³åŒºé–“ã§ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
            segment_groups = self._group_segments_by_silence(segments, min_segment_gap)
            print(f"ğŸ”— ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚°ãƒ«ãƒ¼ãƒ—åŒ–: {len(segment_groups)}ã‚°ãƒ«ãƒ¼ãƒ—")
            
            # ã‚¹ãƒ†ãƒƒãƒ—3: å„ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯ç”Ÿæˆ
            all_vowel_frames = []
            
            for group_idx, group in enumerate(segment_groups):
                group_start = group['start']
                group_end = group['end']
                group_text = group['text']
                group_duration = group_end - group_start
                
                print(f"\n--- ã‚°ãƒ«ãƒ¼ãƒ— {group_idx + 1}/{len(segment_groups)} ---")
                print(f"â±ï¸  æ™‚é–“: {group_start:.2f}s - {group_end:.2f}s ({group_duration:.2f}s)")
                print(f"ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆ: {group_text[:50]}...")
                
                # WAVãƒ‡ãƒ¼ã‚¿ã‚’åˆ‡ã‚Šå‡ºã—
                start_sample = int(group_start * sample_rate)
                end_sample = int(group_end * sample_rate)
                audio_segment = audio_data[start_sample:end_sample]
                
                # ã“ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯ç”Ÿæˆ
                segment_lipsync = self.analyze_text_for_lipsync(
                    text=group_text,
                    audio_data=audio_segment,
                    sample_rate=sample_rate
                )
                
                if segment_lipsync and segment_lipsync.vowel_frames:
                    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«æ™‚é–“ã«å¤‰æ›
                    for frame in segment_lipsync.vowel_frames:
                        adjusted_frame = VowelFrame(
                            timestamp=frame.timestamp + group_start,  # ã‚°ãƒ­ãƒ¼ãƒãƒ«æ™‚é–“ã«å¤‰æ›
                            vowel=frame.vowel,
                            intensity=frame.intensity,
                            duration=frame.duration,
                            is_ending=frame.is_ending
                        )
                        all_vowel_frames.append(adjusted_frame)
                    
                    print(f"âœ… {len(segment_lipsync.vowel_frames)}ãƒ•ãƒ¬ãƒ¼ãƒ ç”Ÿæˆ")
                else:
                    print(f"âš ï¸ ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯ç”Ÿæˆå¤±æ•—ã€ã‚¹ã‚­ãƒƒãƒ—")
            
            # ã‚¹ãƒ†ãƒƒãƒ—4: å…¨ä½“ã‚’çµåˆ
            if not all_vowel_frames:
                print("âŒ æœ‰åŠ¹ãªãƒ•ãƒ¬ãƒ¼ãƒ ãŒ1ã¤ã‚‚ã‚ã‚Šã¾ã›ã‚“")
                return None
            
            # ã‚°ãƒ«ãƒ¼ãƒ—é–“ã®ç„¡éŸ³ã‚’è¿½åŠ 
            all_vowel_frames = self._fill_gaps_with_silence(all_vowel_frames, total_duration)
            
            # æœ€çµ‚çš„ãªLipSyncDataã‚’ç”Ÿæˆ
            combined_lipsync = LipSyncData(
                text=text if text else " ".join([g['text'] for g in segment_groups]),
                total_duration=total_duration,
                vowel_frames=all_vowel_frames
            )
            
            print(f"\nâœ… é•·æ™‚é–“ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯ç”Ÿæˆå®Œäº†")
            print(f"   ç·ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {len(all_vowel_frames)}")
            print(f"   ç·æ™‚é–“: {total_duration:.2f}ç§’")
            print(f"   ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°: {len(segment_groups)}")
            
            silence_count = sum(1 for f in all_vowel_frames if f.vowel == 'sil')
            print(f"   ç„¡éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ : {silence_count}å€‹ ({silence_count/len(all_vowel_frames)*100:.1f}%)")
            
            return combined_lipsync
            
        except Exception as e:
            print(f"âŒ é•·æ™‚é–“WAVè§£æã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()
            return None


    def _get_whisper_segments(self, wav_path: str, provided_text: str = None) -> List[Dict]:
        """Whisperã§ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæƒ…å ±ã‚’å–å¾—
        
        Args:
            wav_path: WAVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            provided_text: æä¾›ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆï¼ˆNoneã®å ´åˆã¯è‡ªå‹•æ–‡å­—èµ·ã“ã—ï¼‰
            
        Returns:
            ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ [{'start': float, 'end': float, 'text': str}, ...]
        """
        try:
            # WhisperTranscriberã‚’ä½¿ç”¨
            if not self.phoneme_analyzer:
                print("âš ï¸ WhisperTranscriberåˆ©ç”¨ä¸å¯")
                return []
            
            # whisper_transcriberãŒã‚ã‚‹ã‹ç¢ºèª
            if not hasattr(self, 'whisper_transcriber'):
                from .whisper_transcriber import WhisperTranscriber
                self.whisper_transcriber = WhisperTranscriber(model_size="medium", device="cuda")
            
            # æ–‡å­—èµ·ã“ã—å®Ÿè¡Œ
            success, transcribed_text, segments = self.whisper_transcriber.transcribe_wav(
                wav_path,
                language="ja"
            )
            
            if not success or not segments:
                print("âš ï¸ Whisperæ–‡å­—èµ·ã“ã—å¤±æ•—")
                return []
            
            print(f"ğŸ“ Whisperæ–‡å­—èµ·ã“ã—çµæœ: {len(transcribed_text)}æ–‡å­—")
            
            return segments
            
        except Exception as e:
            print(f"âŒ Whisperã‚»ã‚°ãƒ¡ãƒ³ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()
            return []


    def _group_segments_by_silence(self, segments: List[Dict], 
                                min_gap: float) -> List[Dict]:
        """ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ç„¡éŸ³åŒºé–“ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        
        Args:
            segments: Whisperã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ
            min_gap: ã‚°ãƒ«ãƒ¼ãƒ—åˆ†å‰²ã®æœ€å°ç„¡éŸ³æ™‚é–“ï¼ˆç§’ï¼‰
            
        Returns:
            ã‚°ãƒ«ãƒ¼ãƒ—ãƒªã‚¹ãƒˆ [{'start': float, 'end': float, 'text': str, 'segments': [...]}, ...]
        """
        if not segments:
            return []
        
        groups = []
        current_group = {
            'start': segments[0]['start'],
            'end': segments[0]['end'],
            'text': segments[0]['text'],
            'segments': [segments[0]]
        }
        
        for i in range(1, len(segments)):
            prev_segment = segments[i - 1]
            curr_segment = segments[i]
            
            # å‰ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã¨ã®é–“éš”
            gap = curr_segment['start'] - prev_segment['end']
            
            if gap >= min_gap:
                # ç„¡éŸ³ãŒé•·ã„ â†’ æ–°ã—ã„ã‚°ãƒ«ãƒ¼ãƒ—ã‚’é–‹å§‹
                groups.append(current_group)
                
                current_group = {
                    'start': curr_segment['start'],
                    'end': curr_segment['end'],
                    'text': curr_segment['text'],
                    'segments': [curr_segment]
                }
                
                print(f"  ğŸ”— ã‚°ãƒ«ãƒ¼ãƒ—åˆ†å‰²: {gap:.2f}ç§’ã®ç„¡éŸ³ã‚’æ¤œå‡º")
            else:
                # åŒã˜ã‚°ãƒ«ãƒ¼ãƒ—ã«è¿½åŠ 
                current_group['end'] = curr_segment['end']
                current_group['text'] += curr_segment['text']
                current_group['segments'].append(curr_segment)
        
        # æœ€å¾Œã®ã‚°ãƒ«ãƒ¼ãƒ—ã‚’è¿½åŠ 
        groups.append(current_group)
        
        return groups


    def _fill_gaps_with_silence(self, vowel_frames: List[VowelFrame], 
                                total_duration: float) -> List[VowelFrame]:
        """ãƒ•ãƒ¬ãƒ¼ãƒ é–“ã®ã‚®ãƒ£ãƒƒãƒ—ã‚’ç„¡éŸ³ã§åŸ‹ã‚ã‚‹
        
        Args:
            vowel_frames: å…ƒã®æ¯éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ãƒªã‚¹ãƒˆï¼ˆæ™‚ç³»åˆ—é †ï¼‰
            total_duration: ç·æ™‚é–“
            
        Returns:
            ã‚®ãƒ£ãƒƒãƒ—ãŒåŸ‹ã‚ã‚‰ã‚ŒãŸæ¯éŸ³ãƒ•ãƒ¬ãƒ¼ãƒ ãƒªã‚¹ãƒˆ
        """
        if not vowel_frames:
            return []
        
        filled_frames = []
        
        # æœ€åˆã®ãƒ•ãƒ¬ãƒ¼ãƒ ã®å‰ã«ç„¡éŸ³ãŒã‚ã‚‹å ´åˆ
        if vowel_frames[0].timestamp > 0.1:
            silence_frame = VowelFrame(
                timestamp=0.0,
                vowel='sil',
                intensity=0.0,
                duration=vowel_frames[0].timestamp,
                is_ending=False
            )
            filled_frames.append(silence_frame)
            print(f"  ğŸ”‡ å…ˆé ­ç„¡éŸ³è¿½åŠ : 0.0s - {vowel_frames[0].timestamp:.2f}s")
        
        # ãƒ•ãƒ¬ãƒ¼ãƒ é–“ã®ã‚®ãƒ£ãƒƒãƒ—ã‚’ãƒã‚§ãƒƒã‚¯
        for i in range(len(vowel_frames)):
            filled_frames.append(vowel_frames[i])
            
            # æ¬¡ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã¨ã®ã‚®ãƒ£ãƒƒãƒ—
            if i < len(vowel_frames) - 1:
                current_end = vowel_frames[i].timestamp + vowel_frames[i].duration
                next_start = vowel_frames[i + 1].timestamp
                gap = next_start - current_end
                
                # 0.1ç§’ä»¥ä¸Šã®ã‚®ãƒ£ãƒƒãƒ—ãŒã‚ã‚‹å ´åˆã€ç„¡éŸ³ã§åŸ‹ã‚ã‚‹
                if gap > 0.1:
                    silence_frame = VowelFrame(
                        timestamp=current_end,
                        vowel='sil',
                        intensity=0.0,
                        duration=gap,
                        is_ending=False
                    )
                    filled_frames.append(silence_frame)
                    print(f"  ğŸ”‡ ç„¡éŸ³è¿½åŠ : {current_end:.2f}s - {next_start:.2f}s ({gap:.2f}s)")
        
        # æœ€å¾Œã®ãƒ•ãƒ¬ãƒ¼ãƒ ã®å¾Œã«ç„¡éŸ³ãŒã‚ã‚‹å ´åˆ
        last_frame = vowel_frames[-1]
        last_end = last_frame.timestamp + last_frame.duration
        if last_end < total_duration - 0.1:
            silence_frame = VowelFrame(
                timestamp=last_end,
                vowel='sil',
                intensity=0.0,
                duration=total_duration - last_end,
                is_ending=False
            )
            filled_frames.append(silence_frame)
            print(f"  ğŸ”‡ æœ«å°¾ç„¡éŸ³è¿½åŠ : {last_end:.2f}s - {total_duration:.2f}s")
        
        return filled_frames


    def _fallback_long_wav_analysis(self, audio_data: np.ndarray, sample_rate: int,
                                    text: str = None) -> Optional[LipSyncData]:
        """Whisperå¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
        
        Args:
            audio_data: éŸ³å£°ãƒ‡ãƒ¼ã‚¿
            sample_rate: ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆ
            text: ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            LipSyncData
        """
        print("âš ï¸ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰: å˜ç´”åˆ†å‰²å‡¦ç†")
        
        if text is None:
            text = "éŸ³å£°è§£æ"
        
        # å…¨ä½“ã‚’ä¸€åº¦ã«å‡¦ç†ï¼ˆæ—¢å­˜ã®æ–¹æ³•ï¼‰
        return self.analyze_text_for_lipsync(text, audio_data, sample_rate)